{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4420a1cc",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Algorithmic Trading\n",
    "## Step-by-Step Implementation of DQN (Fixed Version)\n",
    "This notebook improves upon the previous DQN implementation by fixing key issues:\n",
    "\n",
    "- **Better Reward Function**: Encourages profitable trades instead of just holding cash.\n",
    "- **Increased Exploration**: Ensures the agent explores more during training.\n",
    "- **Hold Penalty**: Prevents the agent from always holding cash.\n",
    "- **Longer Training Time**: Allows more learning for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdf9e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym import spaces\n",
    "\n",
    "# Load preprocessed data\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Convert datadate to datetime format\n",
    "train_df['datadate'] = pd.to_datetime(train_df['datadate'])\n",
    "test_df['datadate'] = pd.to_datetime(test_df['datadate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49604e4",
   "metadata": {},
   "source": [
    "## Step 1: Define the Fixed Trading Environment\n",
    "We improve the environment by:\n",
    "- **Fixing the Reward Function** to reward profitable trades.\n",
    "- **Adding a Hold Penalty** to discourage inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a1d048c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample state: [   27.98        29.451279 10000.           0.      ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class MeanReversionTradingEnv(gym.Env):\n",
    "    def __init__(self, df, window_size=20, initial_cash=10000):\n",
    "        super(MeanReversionTradingEnv, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.initial_cash = initial_cash\n",
    "        self.current_step = 0\n",
    "        self.cash = initial_cash\n",
    "        self.shares_held = 0\n",
    "        self.total_value = initial_cash  # Track total portfolio value\n",
    "\n",
    "        # Action space: 0 = Buy, 1 = Hold, 2 = Sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation space: [current price, moving average, cash, shares held]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=np.inf, shape=(4,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment for a new episode\"\"\"\n",
    "        self.current_step = self.window_size  # Start after enough data points\n",
    "        self.cash = self.initial_cash\n",
    "        self.shares_held = 0\n",
    "        self.total_value = self.initial_cash\n",
    "        return self._get_observation()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Return the current state representation\"\"\"\n",
    "        current_price = self.df.iloc[self.current_step]['adjcp']\n",
    "        moving_avg = self.df.iloc[self.current_step]['20_day_MA']\n",
    "        return np.array([current_price, moving_avg, self.cash, self.shares_held], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return next state, reward, done flag\"\"\"\n",
    "        current_price = self.df.iloc[self.current_step]['adjcp']\n",
    "        moving_avg = self.df.iloc[self.current_step]['20_day_MA']\n",
    "        \n",
    "        # Fix: Ensure done flag stops at valid indices\n",
    "        done = self.current_step >= len(self.df) - 1  # Stop at the last valid row\n",
    "        reward = 0\n",
    "\n",
    "        # Mean Reversion Logic: Trade only when price deviates significantly from the moving average\n",
    "        if action == 0 and current_price < moving_avg:  # Buy if price < MA\n",
    "            num_shares = self.cash // current_price  # Buy as many as possible\n",
    "            if num_shares > 0:\n",
    "                self.cash -= num_shares * current_price\n",
    "                self.shares_held += num_shares\n",
    "                reward = 1  # Positive reward for buying at a discount\n",
    "\n",
    "        elif action == 2 and current_price > moving_avg and self.shares_held > 0:  # Sell if price > MA\n",
    "            self.cash += self.shares_held * current_price\n",
    "            self.shares_held = 0\n",
    "            reward = 1  # Positive reward for selling at a premium\n",
    "\n",
    "        elif action == 2 and self.shares_held == 0:\n",
    "            reward = -1  # Penalize selling when no shares are held\n",
    "\n",
    "        # Update portfolio value\n",
    "        self.total_value = self.cash + (self.shares_held * current_price)\n",
    "\n",
    "        # Move to the next step\n",
    "        if not done:  # Only increment if not done\n",
    "            self.current_step += 1\n",
    "            \n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Print current state for debugging\"\"\"\n",
    "        print(f\"Step: {self.current_step}, Cash: {self.cash}, Shares: {self.shares_held}, Total Value: {self.total_value}\")\n",
    "\n",
    "# Initialize environment\n",
    "env = MeanReversionTradingEnv(train_df)\n",
    "state = env.reset()\n",
    "print(\"Sample state:\", state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce4ac4",
   "metadata": {},
   "source": [
    "## Step 2: Train the Fixed DQN Agent\n",
    "We modify the training process by:\n",
    "- Increasing exploration\n",
    "- Extending training time\n",
    "- Adjusting reward signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf2cd311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -320     |\n",
      "|    exploration_rate | 0.596    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 8976     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 82.8     |\n",
      "|    n_updates        | 2218     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -364     |\n",
      "|    exploration_rate | 0.192    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 17952    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 23.4     |\n",
      "|    n_updates        | 4462     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -413     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 794      |\n",
      "|    time_elapsed     | 33       |\n",
      "|    total_timesteps  | 26928    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 24.8     |\n",
      "|    n_updates        | 6706     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -415     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 782      |\n",
      "|    time_elapsed     | 45       |\n",
      "|    total_timesteps  | 35904    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 46.7     |\n",
      "|    n_updates        | 8950     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -413     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 776      |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total_timesteps  | 44880    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 63.5     |\n",
      "|    n_updates        | 11194    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -402     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 69       |\n",
      "|    total_timesteps  | 53856    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 28       |\n",
      "|    n_updates        | 13438    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -386     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 770      |\n",
      "|    time_elapsed     | 81       |\n",
      "|    total_timesteps  | 62832    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.42     |\n",
      "|    n_updates        | 15682    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -370     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 93       |\n",
      "|    total_timesteps  | 71808    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 22.2     |\n",
      "|    n_updates        | 17926    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -360     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total_timesteps  | 80784    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.763    |\n",
      "|    n_updates        | 20170    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -342     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 116      |\n",
      "|    total_timesteps  | 89760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 35       |\n",
      "|    n_updates        | 22414    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.24e+03 |\n",
      "|    ep_rew_mean      | -335     |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total_timesteps  | 98736    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 13.3     |\n",
      "|    n_updates        | 24658    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialize improved DQN model\n",
    "model = DQN(\n",
    "    'MlpPolicy', env, verbose=1, learning_rate=0.001,\n",
    "    buffer_size=10000, batch_size=32, exploration_fraction=0.2,  # More exploration\n",
    "    exploration_final_eps=0.1  # Allow some randomness\n",
    ")\n",
    "\n",
    "# Train the model for longer\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('dqn_trading_model_fixed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034a8ee",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the Fixed Model\n",
    "We test the trained model on **unseen test data (2019-2021)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af9bf63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\.venv\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "c:\\Users\\caleb\\anaconda3\\envs\\.venv\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -390.0, Std Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "model = DQN.load('dqn_trading_model_fixed')\n",
    "\n",
    "# Initialize test environment\n",
    "test_env = MeanReversionTradingEnv(test_df)\n",
    "mean_reward, std_reward = evaluate_policy(model, test_env, n_eval_episodes=10)\n",
    "\n",
    "print(f\"Mean Reward: {mean_reward}, Std Reward: {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ba4e1",
   "metadata": {},
   "source": [
    "## Step 4: Run the Fixed Agent on Test Data\n",
    "Let's visualize how the improved agent performs in the test environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae77206f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MeanReversionTradingEnv' object has no attribute 'holdings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m cash_available \u001b[38;5;241m=\u001b[39m test_env\u001b[38;5;241m.\u001b[39mcash  \u001b[38;5;66;03m# Assuming test_env has a cash attribute\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m stock_holdings \u001b[38;5;241m=\u001b[39m \u001b[43mtest_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mholdings\u001b[49m  \u001b[38;5;66;03m# Assuming test_env tracks shares\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:    \n\u001b[0;32m      8\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MeanReversionTradingEnv' object has no attribute 'holdings'"
     ]
    }
   ],
   "source": [
    "# Reset environment\n",
    "obs = test_env.reset()\n",
    "done = False\n",
    "cash_available = test_env.cash  # Assuming test_env has a cash attribute\n",
    "stock_holdings = test_env.shares_held  # Assuming test_env tracks shares\n",
    "\n",
    "while not done:    \n",
    "    action, _ = model.predict(obs)\n",
    "\n",
    "    # Prevent selling if no stocks are held\n",
    "    if action == 2 and stock_holdings == 0:\n",
    "        action = 1  # Hold instead of selling\n",
    "\n",
    "    obs, reward, done, _ = test_env.step(action)\n",
    "\n",
    "    # Update cash and stock holdings\n",
    "    cash_available = test_env.cash\n",
    "    stock_holdings = test_env.shares_held\n",
    "\n",
    "    # If a trade is made, render and print\n",
    "    if action == 0 or action == 2:\n",
    "        print(f\"Trade executed at step {test_env.current_step}: Action {action}\")\n",
    "\n",
    "    # Render every 500 steps\n",
    "    if test_env.current_step % 100 == 0:\n",
    "        test_env.render()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba3d2ba",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- The **reward function** now incentivizes profitable trades.\n",
    "- The agent **actively explores** trading actions.\n",
    "- We **increased training time** to improve learning.\n",
    "- Next steps: Compare DQN with **PPO and SAC** for even better performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
