{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c99581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False             \n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        # print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('./working/account_value_train.png') # get rid of leading '\\'\n",
    "            plt.close()\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            \n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('./working/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            #print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('/kaggle./working/account_rewards_train.csv')\n",
    "            \n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #actions = (actions.astype(int))\n",
    "            \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ff05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "\n",
    "# turbulence index: 90-150 reasonable threshold\n",
    "#TURBULENCE_THRESHOLD = 140\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrade(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0,turbulence_threshold=140\n",
    "                 ,initial=True, previous_state=[], model_name='', iteration=''):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "        self.model_name=model_name        \n",
    "        self.iteration=iteration\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence<self.turbulence_threshold:\n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += \\\n",
    "                self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 (1- TRANSACTION_FEE_PERCENT)\n",
    "                \n",
    "                self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just clear out all positions \n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              (1- TRANSACTION_FEE_PERCENT)\n",
    "                self.state[index+STOCK_DIM+1] =0\n",
    "                self.cost += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence< self.turbulence_threshold:\n",
    "            available_amount = self.state[0] // self.state[index+1]\n",
    "            # print('available_amount:{}'.format(available_amount))\n",
    "            \n",
    "            #update balance\n",
    "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                              (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "            \n",
    "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just stop buying\n",
    "            pass\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        # print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('./working/account_value_trade_{}_{}.png'.format(self.model_name, self.iteration))\n",
    "            plt.close()\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('./working/account_value_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))- self.asset_memory[0] ))\n",
    "            print(\"total_cost: \", self.cost)\n",
    "            print(\"total trades: \", self.trades)\n",
    "\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            \n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            df_rewards.to_csv('./working/account_rewards_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
    "            \n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #actions = (actions.astype(int))\n",
    "            if self.turbulence>=self.turbulence_threshold:\n",
    "                actions=np.array([-HMAX_NORMALIZE]*STOCK_DIM)\n",
    "                \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            self.turbulence = self.data['turbulence'].values[0]\n",
    "            #print(self.turbulence)\n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):  \n",
    "        if self.initial:\n",
    "            self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "            self.day = 0\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.turbulence = 0\n",
    "            self.cost = 0\n",
    "            self.trades = 0\n",
    "            self.terminal = False \n",
    "            #self.iteration=self.iteration\n",
    "            self.rewards_memory = []\n",
    "            #initiate state\n",
    "            self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                          self.data.adjcp.values.tolist() + \\\n",
    "                          [0]*STOCK_DIM + \\\n",
    "                          self.data.macd.values.tolist() + \\\n",
    "                          self.data.rsi.values.tolist()  + \\\n",
    "                          self.data.cci.values.tolist()  + \\\n",
    "                          self.data.adx.values.tolist() \n",
    "        else:\n",
    "            previous_total_asset = self.previous_state[0]+ \\\n",
    "            sum(np.array(self.previous_state[1:(STOCK_DIM+1)])*np.array(self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "            #self.asset_memory = [self.previous_state[0]]\n",
    "            self.day = 0\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.turbulence = 0\n",
    "            self.cost = 0\n",
    "            self.trades = 0\n",
    "            self.terminal = False \n",
    "            #self.iteration=iteration\n",
    "            self.rewards_memory = []\n",
    "            #initiate state\n",
    "            #self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]\n",
    "            #[0]*STOCK_DIM + \\\n",
    "\n",
    "            self.state = [ self.previous_state[0]] + \\\n",
    "                          self.data.adjcp.values.tolist() + \\\n",
    "                          self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]+ \\\n",
    "                          self.data.macd.values.tolist() + \\\n",
    "                          self.data.rsi.values.tolist()  + \\\n",
    "                          self.data.cci.values.tolist()  + \\\n",
    "                          self.data.adx.values.tolist() \n",
    "            \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eae39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "\n",
    "# turbulence index: 90-150 reasonable threshold\n",
    "#TURBULENCE_THRESHOLD = 140\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvValidation(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, day = 0, turbulence_threshold=140, iteration=''):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "        \n",
    "        self.iteration=iteration\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence<self.turbulence_threshold:\n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += \\\n",
    "                self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 (1- TRANSACTION_FEE_PERCENT)\n",
    "                \n",
    "                self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just clear out all positions \n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              (1- TRANSACTION_FEE_PERCENT)\n",
    "                self.state[index+STOCK_DIM+1] =0\n",
    "                self.cost += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence< self.turbulence_threshold:\n",
    "            available_amount = self.state[0] // self.state[index+1]\n",
    "            # print('available_amount:{}'.format(available_amount))\n",
    "            \n",
    "            #update balance\n",
    "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                              (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "            \n",
    "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just stop buying\n",
    "            pass\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        # print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('./working/account_value_validation_{}.png'.format(self.iteration))\n",
    "            plt.close()\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('./working/account_value_validation_{}.csv'.format(self.iteration))\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- self.asset_memory[0] ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total trades: \", self.trades)\n",
    "\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            #print(\"Sharpe: \",sharpe)\n",
    "            \n",
    "            #df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('/kaggle./working/account_rewards_trade_{}.csv'.format(self.iteration))\n",
    "            \n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #actions = (actions.astype(int))\n",
    "            if self.turbulence>=self.turbulence_threshold:\n",
    "                actions=np.array([-HMAX_NORMALIZE]*STOCK_DIM)\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            self.turbulence = self.data['turbulence'].values[0]\n",
    "            #print(self.turbulence)\n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):  \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        #self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist()  + \\\n",
    "                      self.data.cci.values.tolist()  + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "            \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smooth-clothing",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-03T04:57:54.198152Z",
     "iopub.status.busy": "2021-04-03T04:57:54.197005Z",
     "iopub.status.idle": "2021-04-03T05:00:42.175245Z",
     "shell.execute_reply": "2021-04-03T05:00:42.173533Z"
    },
    "papermill": {
     "duration": 167.993805,
     "end_time": "2021-04-03T05:00:42.175556",
     "exception": false,
     "start_time": "2021-04-03T04:57:54.181751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#from env.EnvMultipleStock_train import StockEnvTrain\n",
    "#from env.EnvMultipleStock_validation import StockEnvValidation\n",
    "#from env.EnvMultipleStock_trade import StockEnvTrade\n",
    "\n",
    "#!pip uninstall -y tensorflow-probability\n",
    "#!pip uninstall -y tensorflow-cloud\n",
    "#!pip uninstall -y pytorch-lightning\n",
    "#!pip uninstall -y tensorflow\n",
    "#!pip uninstall -y gast\n",
    "\n",
    "#!pip install -qq 'tensorflow==1.15.0'\n",
    "import tensorflow as tf\n",
    "\n",
    "#!apt-get update > /dev/null\n",
    "#!apt-get install -qq -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
    "#!pip install -qq \"stable-baselines[mpi]==2.9.0\"\n",
    "\n",
    "from stable_baselines import GAIL, SAC\n",
    "from stable_baselines import ACER\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines import TD3\n",
    "\n",
    "from stable_baselines.ddpg.policies import DDPGPolicy\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\n",
    "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-calcium",
   "metadata": {
    "papermill": {
     "duration": 0.035776,
     "end_time": "2021-04-03T05:00:42.246099",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.210323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"dataset\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Dataset\n",
    "        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "registered-specialist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:42.319007Z",
     "iopub.status.busy": "2021-04-03T05:00:42.318274Z",
     "iopub.status.idle": "2021-04-03T05:00:42.766545Z",
     "shell.execute_reply": "2021-04-03T05:00:42.765961Z"
    },
    "papermill": {
     "duration": 0.486202,
     "end_time": "2021-04-03T05:00:42.766718",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.280516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datadate</th>\n",
       "      <th>tic</th>\n",
       "      <th>adjcp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>cci</th>\n",
       "      <th>adx</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20090102</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>12.964286</td>\n",
       "      <td>12.268571</td>\n",
       "      <td>13.005714</td>\n",
       "      <td>12.165714</td>\n",
       "      <td>26641980.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20090102</td>\n",
       "      <td>AXP</td>\n",
       "      <td>19.330000</td>\n",
       "      <td>18.570000</td>\n",
       "      <td>19.520000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>10955620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20090102</td>\n",
       "      <td>BA</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>42.800000</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>42.780000</td>\n",
       "      <td>7010171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20090102</td>\n",
       "      <td>CAT</td>\n",
       "      <td>46.910000</td>\n",
       "      <td>44.910000</td>\n",
       "      <td>46.980000</td>\n",
       "      <td>44.710000</td>\n",
       "      <td>7116726.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20090102</td>\n",
       "      <td>CSCO</td>\n",
       "      <td>16.960000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>40977480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  datadate   tic      adjcp       open       high        low  \\\n",
       "0           0  20090102  AAPL  12.964286  12.268571  13.005714  12.165714   \n",
       "1           1  20090102   AXP  19.330000  18.570000  19.520000  18.400000   \n",
       "2           2  20090102    BA  45.250000  42.800000  45.560000  42.780000   \n",
       "3           3  20090102   CAT  46.910000  44.910000  46.980000  44.710000   \n",
       "4           4  20090102  CSCO  16.960000  16.410000  17.000000  16.250000   \n",
       "\n",
       "       volume  macd    rsi        cci    adx  turbulence  \n",
       "0  26641980.0   0.0  100.0  66.666667  100.0         0.0  \n",
       "1  10955620.0   0.0  100.0  66.666667  100.0         0.0  \n",
       "2   7010171.0   0.0  100.0  66.666667  100.0         0.0  \n",
       "3   7116726.0   0.0    0.0  66.666667  100.0         0.0  \n",
       "4  40977480.0   0.0  100.0  66.666667  100.0         0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '/kaggle/input/trading/trading.csv'\n",
    "path = 'trading.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "understanding-fields",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:42.841646Z",
     "iopub.status.busy": "2021-04-03T05:00:42.841006Z",
     "iopub.status.idle": "2021-04-03T05:00:42.845211Z",
     "shell.execute_reply": "2021-04-03T05:00:42.844583Z"
    },
    "papermill": {
     "duration": 0.043923,
     "end_time": "2021-04-03T05:00:42.845383",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.801460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63\n",
    "validation_window = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cheap-madison",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:42.927058Z",
     "iopub.status.busy": "2021-04-03T05:00:42.925790Z",
     "iopub.status.idle": "2021-04-03T05:00:42.938471Z",
     "shell.execute_reply": "2021-04-03T05:00:42.937753Z"
    },
    "papermill": {
     "duration": 0.055925,
     "end_time": "2021-04-03T05:00:42.938636",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.882711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20151002 20151005 20151006 ... 20200702 20200706 20200707]\n"
     ]
    }
   ],
   "source": [
    "unique_trade_date = df[(df.datadate > 20151001)&(df.datadate <= 20200707)].datadate.unique()\n",
    "print(unique_trade_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-lease",
   "metadata": {
    "papermill": {
     "duration": 0.036451,
     "end_time": "2021-04-03T05:00:43.010626",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.974175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"stable\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Stable Baseline\n",
    "        <a class=\"anchor-link\" href=\"#stable\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "senior-dispatch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.101017Z",
     "iopub.status.busy": "2021-04-03T05:00:43.100132Z",
     "iopub.status.idle": "2021-04-03T05:00:43.103435Z",
     "shell.execute_reply": "2021-04-03T05:00:43.102911Z"
    },
    "papermill": {
     "duration": 0.056248,
     "end_time": "2021-04-03T05:00:43.103598",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.047350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_A2C(env_train, model_name, timesteps=25000):\n",
    "    start = time.time()\n",
    "    model = A2C('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (A2C): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_ACER(env_train, model_name, timesteps=25000):\n",
    "    start = time.time()\n",
    "    model = ACER('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (A2C): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_DDPG(env_train, model_name, timesteps=10000):\n",
    "    # add the noise objects for DDPG\n",
    "    n_actions = env_train.action_space.shape[-1]\n",
    "    param_noise = None\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    "\n",
    "    start = time.time()\n",
    "    model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (DDPG): ', (end-start)/60,' minutes')\n",
    "    return model\n",
    "\n",
    "def train_PPO(env_train, model_name, timesteps=50000):\n",
    "    start = time.time()\n",
    "    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_GAIL(env_train, model_name, timesteps=1000):\n",
    "    start = time.time()\n",
    "    # generate expert trajectories\n",
    "    model = SAC('MLpPolicy', env_train, verbose=1)\n",
    "    GAIL.generate_expert_traj(model, 'expert_model_gail', n_timesteps=100, n_episodes=10)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = GAIL.ExpertDataset(expert_path='expert_model_gail.npz', traj_limitation=10, verbose=1)\n",
    "    model = GAIL('MLpPolicy', env_train, dataset, verbose=1)\n",
    "\n",
    "    model.learn(total_timesteps=1000)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-camera",
   "metadata": {
    "papermill": {
     "duration": 0.035164,
     "end_time": "2021-04-03T05:00:43.174458",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.139294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"additional\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Additional Functions\n",
    "        <a class=\"anchor-link\" href=\"#additional\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complex-berkeley",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.254427Z",
     "iopub.status.busy": "2021-04-03T05:00:43.253753Z",
     "iopub.status.idle": "2021-04-03T05:00:43.257932Z",
     "shell.execute_reply": "2021-04-03T05:00:43.257250Z"
    },
    "papermill": {
     "duration": 0.048232,
     "end_time": "2021-04-03T05:00:43.258157",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.209925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_split(df,start,end):\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data=data.sort_values(['datadate','tic'],ignore_index=True)\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "    return data\n",
    "\n",
    "def get_validation_sharpe(iteration):\n",
    "    df_total_value = pd.read_csv('./working/account_value_validation_{}.csv'.format(iteration), index_col=0)\n",
    "    df_total_value.columns = ['account_value_train']\n",
    "    df_total_value['daily_return'] = df_total_value.pct_change(1)\n",
    "    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\n",
    "             df_total_value['daily_return'].std()\n",
    "    return sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-sunrise",
   "metadata": {
    "papermill": {
     "duration": 0.034922,
     "end_time": "2021-04-03T05:00:43.328501",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.293579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"predvalid\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Predict-Validate\n",
    "        <a class=\"anchor-link\" href=\"#predvalid\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stock-robert",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.411283Z",
     "iopub.status.busy": "2021-04-03T05:00:43.410568Z",
     "iopub.status.idle": "2021-04-03T05:00:43.414371Z",
     "shell.execute_reply": "2021-04-03T05:00:43.413801Z"
    },
    "papermill": {
     "duration": 0.05106,
     "end_time": "2021-04-03T05:00:43.414516",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.363456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DRL_prediction(df,\n",
    "                   model,\n",
    "                   name,\n",
    "                   last_state,\n",
    "                   iter_num,\n",
    "                   unique_trade_date,\n",
    "                   rebalance_window,\n",
    "                   turbulence_threshold,\n",
    "                   initial):\n",
    "\n",
    "    trade_data = data_split(df, start=unique_trade_date[iter_num - rebalance_window], end=unique_trade_date[iter_num])\n",
    "    env_trade = DummyVecEnv([lambda: StockEnvTrade(trade_data,\n",
    "                                                   turbulence_threshold=turbulence_threshold,\n",
    "                                                   initial=initial,\n",
    "                                                   previous_state=last_state,\n",
    "                                                   model_name=name,\n",
    "                                                   iteration=iter_num)])\n",
    "    obs_trade = env_trade.reset()\n",
    "\n",
    "    for i in range(len(trade_data.index.unique())):\n",
    "        action, _states = model.predict(obs_trade)\n",
    "        obs_trade, rewards, dones, info = env_trade.step(action)\n",
    "        if i == (len(trade_data.index.unique()) - 2):\n",
    "            last_state = env_trade.render()\n",
    "\n",
    "    df_last_state = pd.DataFrame({'last_state': last_state})\n",
    "    df_last_state.to_csv('./working/last_state_{}_{}.csv'.format(name, i), index=False)\n",
    "    return last_state\n",
    "\n",
    "def DRL_validation(model, test_data, test_env, test_obs) -> None:\n",
    "    for i in range(len(test_data.index.unique())):\n",
    "        action, _states = model.predict(test_obs)\n",
    "        test_obs, rewards, dones, info = test_env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-anthropology",
   "metadata": {
    "papermill": {
     "duration": 0.036259,
     "end_time": "2021-04-03T05:00:43.486375",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.450116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"ensemble\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Ensemble\n",
    "        <a class=\"anchor-link\" href=\"#ensemble\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrapped-pathology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.582758Z",
     "iopub.status.busy": "2021-04-03T05:00:43.581780Z",
     "iopub.status.idle": "2021-04-03T05:00:43.585359Z",
     "shell.execute_reply": "2021-04-03T05:00:43.584799Z"
    },
    "papermill": {
     "duration": 0.063935,
     "end_time": "2021-04-03T05:00:43.585499",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.521564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_ensemble_strategy(df, unique_trade_date, rebalance_window, validation_window) -> None:\n",
    "    last_state_ensemble = []\n",
    "    ppo_sharpe_list = []\n",
    "    ddpg_sharpe_list = []\n",
    "    a2c_sharpe_list = []\n",
    "\n",
    "    model_use = []\n",
    "\n",
    "    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n",
    "    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n",
    "    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n",
    "        if i - rebalance_window - validation_window == 0:\n",
    "            # inital state\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "\n",
    "        # Tuning trubulence index based on historical data\n",
    "        # Turbulence lookback window is one quarter\n",
    "        end_date_index = df.index[df[\"datadate\"] == unique_trade_date[i - rebalance_window - validation_window]].to_list()[-1]\n",
    "        start_date_index = end_date_index - validation_window*30 + 1\n",
    "\n",
    "        historical_turbulence = df.iloc[start_date_index:(end_date_index + 1), :]\n",
    "        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n",
    "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)\n",
    "\n",
    "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
    "            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n",
    "            # then we assume that the current market is volatile,\n",
    "            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold\n",
    "            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n",
    "            turbulence_threshold = insample_turbulence_threshold\n",
    "        else:\n",
    "            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n",
    "            # then we tune up the turbulence_threshold, meaning we lower the risk\n",
    "            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        print(\" - Turbulence_threshold: \", turbulence_threshold)\n",
    "\n",
    "        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n",
    "        env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "\n",
    "        ## validation env\n",
    "        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n",
    "                                end=unique_trade_date[i - rebalance_window])\n",
    "        env_val = DummyVecEnv([lambda: StockEnvValidation(validation,\n",
    "                                                          turbulence_threshold=turbulence_threshold,\n",
    "                                                          iteration=i)])\n",
    "        obs_val = env_val.reset()\n",
    "        \n",
    "        print(\" - Model training from: \", 20090000, \"to \",\n",
    "              unique_trade_date[i - rebalance_window - validation_window])\n",
    "        print(\" - A2C Training\")\n",
    "        model_a2c = train_A2C(env_train, model_name=\"A2C_30k_dow_{}\".format(i), timesteps=30000)\n",
    "        print(\" - A2C Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_a2c, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_a2c = get_validation_sharpe(i)\n",
    "        print(\" - A2C Sharpe Ratio: \", sharpe_a2c)\n",
    "\n",
    "        print(\" - PPO Training\")\n",
    "        model_ppo = train_PPO(env_train, model_name=\"PPO_100k_dow_{}\".format(i), timesteps=100000)\n",
    "        print(\" - PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ppo = get_validation_sharpe(i)\n",
    "        print(\" - PPO Sharpe Ratio: \", sharpe_ppo)\n",
    "\n",
    "        print(\" - DDPG Training\")\n",
    "        model_ddpg = train_DDPG(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=10000)\n",
    "        print(\" - DDPG Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        \n",
    "        DRL_validation(model=model_ddpg, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ddpg = get_validation_sharpe(i)\n",
    "\n",
    "        ppo_sharpe_list.append(sharpe_ppo)\n",
    "        a2c_sharpe_list.append(sharpe_a2c)\n",
    "        ddpg_sharpe_list.append(sharpe_ddpg)\n",
    "\n",
    "        # Model Selection based on sharpe ratio\n",
    "        if (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n",
    "            model_ensemble = model_ppo\n",
    "            model_use.append('PPO')\n",
    "        elif (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n",
    "            model_ensemble = model_a2c\n",
    "            model_use.append('A2C')\n",
    "        else:\n",
    "            model_ensemble = model_ddpg\n",
    "            model_use.append('DDPG')\n",
    "\n",
    "        print(\" - Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n",
    "        print(\"-\" * 50)\n",
    "        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n",
    "                                             last_state=last_state_ensemble, iter_num=i,\n",
    "                                             unique_trade_date=unique_trade_date,\n",
    "                                             rebalance_window=rebalance_window,\n",
    "                                             turbulence_threshold=turbulence_threshold,\n",
    "                                             initial=initial)\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371b7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'c:\\\\Users\\\\willr\\\\Classes\\\\Spring 2025\\\\CS-551\\\\Research-Paper\\\\Repo\\\\cs451': ['.git', '.gitignore', 'env', 'old_stuff', 'README.md', 'stocks-reinforcement-learning-ensemble.ipynb', 'test.py', 'trading.csv', 'working']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wired-burlington",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.663960Z",
     "iopub.status.busy": "2021-04-03T05:00:43.662743Z",
     "iopub.status.idle": "2021-04-03T07:46:32.346383Z",
     "shell.execute_reply": "2021-04-03T07:46:32.345691Z"
    },
    "papermill": {
     "duration": 9948.72573,
     "end_time": "2021-04-03T07:46:32.346561",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.620831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20151002\n",
      " - A2C Training\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:160: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:449: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:194: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\a2c\\a2c.py:196: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      " - Training time (A2C):  0.8930553197860718  minutes\n",
      " - A2C Validation from:  20151002 to  20160104\n",
      " - A2C Sharpe Ratio:  0.0605294524241326\n",
      " - PPO Training\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      " - Training time (PPO):  3.4502811551094057  minutes\n",
      " - PPO Validation from:  20151002 to  20160104\n",
      " - PPO Sharpe Ratio:  0.041161212687261535\n",
      " - DDPG Training\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\ddpg\\policies.py:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\ddpg\\ddpg.py:94: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\ddpg\\ddpg.py:444: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\willr\\miniconda3\\envs\\sb_env\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:432: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      " - Training time (DDPG):  0.4557702700297038  minutes\n",
      " - DDPG Validation from:  20151002 to  20160104\n",
      " - Trading from:  20160104 to  20160405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1000000\n",
      "end_total_asset:1061705.7927371692\n",
      "total_reward:61705.79273716919\n",
      "total_cost:  3892.2639331684927\n",
      "total trades:  1234\n",
      "Sharpe:  0.19336218516479747\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20160104\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9397946317990621  minutes\n",
      " - A2C Validation from:  20160104 to  20160405\n",
      " - A2C Sharpe Ratio:  0.1302796897700097\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.0630728721618654  minutes\n",
      " - PPO Validation from:  20160104 to  20160405\n",
      " - PPO Sharpe Ratio:  0.09034012890368977\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4683325926462809  minutes\n",
      " - DDPG Validation from:  20160104 to  20160405\n",
      " - Trading from:  20160405 to  20160705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1061705.7927371692\n",
      "end_total_asset:1027834.0332132671\n",
      "total_reward:-33871.75952390209\n",
      "total_cost:  4475.004094604928\n",
      "total trades:  1505\n",
      "Sharpe:  -0.10212597312592789\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20160405\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.882175068060557  minutes\n",
      " - A2C Validation from:  20160405 to  20160705\n",
      " - A2C Sharpe Ratio:  0.02692374959359747\n",
      " - PPO Training\n",
      " - Training time (PPO):  4.089745752016703  minutes\n",
      " - PPO Validation from:  20160405 to  20160705\n",
      " - PPO Sharpe Ratio:  -0.01544848238184867\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.654392929871877  minutes\n",
      " - DDPG Validation from:  20160405 to  20160705\n",
      " - Trading from:  20160705 to  20161003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1027834.0332132671\n",
      "end_total_asset:1046475.5652065036\n",
      "total_reward:18641.53199323651\n",
      "total_cost:  580.3961247405496\n",
      "total trades:  1025\n",
      "Sharpe:  0.09457025026949122\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20160705\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9559350887934367  minutes\n",
      " - A2C Validation from:  20160705 to  20161003\n",
      " - A2C Sharpe Ratio:  -0.0068626726363601\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.4951836864153543  minutes\n",
      " - PPO Validation from:  20160705 to  20161003\n",
      " - PPO Sharpe Ratio:  -0.04569879502728464\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4888952294985453  minutes\n",
      " - DDPG Validation from:  20160705 to  20161003\n",
      " - Trading from:  20161003 to  20170103\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1046475.5652065036\n",
      "end_total_asset:1129562.9359116885\n",
      "total_reward:83087.37070518488\n",
      "total_cost:  3727.257289335566\n",
      "total trades:  1290\n",
      "Sharpe:  0.5131897778857498\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20161003\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.8905986507733663  minutes\n",
      " - A2C Validation from:  20161003 to  20170103\n",
      " - A2C Sharpe Ratio:  0.48392795337056743\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.075076770782471  minutes\n",
      " - PPO Validation from:  20161003 to  20170103\n",
      " - PPO Sharpe Ratio:  0.5875323291006941\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4958859086036682  minutes\n",
      " - DDPG Validation from:  20161003 to  20170103\n",
      " - Trading from:  20170103 to  20170404\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1129562.9359116885\n",
      "end_total_asset:1152410.885475309\n",
      "total_reward:22847.949563620612\n",
      "total_cost:  3041.089733321215\n",
      "total trades:  1173\n",
      "Sharpe:  0.15712074249642044\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20170103\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9379154284795125  minutes\n",
      " - A2C Validation from:  20170103 to  20170404\n",
      " - A2C Sharpe Ratio:  0.20751828412816464\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.8558170557022096  minutes\n",
      " - PPO Validation from:  20170103 to  20170404\n",
      " - PPO Sharpe Ratio:  0.2047404430286975\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.6346845507621766  minutes\n",
      " - DDPG Validation from:  20170103 to  20170404\n",
      " - Trading from:  20170404 to  20170705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1152410.885475309\n",
      "end_total_asset:1178932.3105331813\n",
      "total_reward:26521.4250578722\n",
      "total_cost:  3682.9207183445224\n",
      "total trades:  992\n",
      "Sharpe:  0.1737163282259548\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20170404\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9231436371803283  minutes\n",
      " - A2C Validation from:  20170404 to  20170705\n",
      " - A2C Sharpe Ratio:  0.15420590471678253\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.388765398661296  minutes\n",
      " - PPO Validation from:  20170404 to  20170705\n",
      " - PPO Sharpe Ratio:  0.1577100744034323\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.49804226160049436  minutes\n",
      " - DDPG Validation from:  20170404 to  20170705\n",
      " - Trading from:  20170705 to  20171003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1178932.3105331813\n",
      "end_total_asset:1205963.544494191\n",
      "total_reward:27031.233961009653\n",
      "total_cost:  3332.7355672256485\n",
      "total trades:  954\n",
      "Sharpe:  0.16869820739479335\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20170705\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9342812379201253  minutes\n",
      " - A2C Validation from:  20170705 to  20171003\n",
      " - A2C Sharpe Ratio:  0.26582072794117934\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.192033008734385  minutes\n",
      " - PPO Validation from:  20170705 to  20171003\n",
      " - PPO Sharpe Ratio:  0.2350837292575495\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.47428056796391804  minutes\n",
      " - DDPG Validation from:  20170705 to  20171003\n",
      " - Trading from:  20171003 to  20180103\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1205963.544494191\n",
      "end_total_asset:1323462.848857289\n",
      "total_reward:117499.304363098\n",
      "total_cost:  1690.246334190255\n",
      "total trades:  1063\n",
      "Sharpe:  0.6319103827085982\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20171003\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.1767703612645468  minutes\n",
      " - A2C Validation from:  20171003 to  20180103\n",
      " - A2C Sharpe Ratio:  0.60058954588091\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.2380985021591187  minutes\n",
      " - PPO Validation from:  20171003 to  20180103\n",
      " - PPO Sharpe Ratio:  0.38745256602797346\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.5024046103159586  minutes\n",
      " - DDPG Validation from:  20171003 to  20180103\n",
      " - Trading from:  20180103 to  20180405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1323462.848857289\n",
      "end_total_asset:1344141.9069425096\n",
      "total_reward:20679.058085220633\n",
      "total_cost:  2172.518119147042\n",
      "total trades:  363\n",
      "Sharpe:  0.11276613511301112\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180103\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.2957510391871134  minutes\n",
      " - A2C Validation from:  20180103 to  20180405\n",
      " - A2C Sharpe Ratio:  -0.01474135660521354\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.7856744170188903  minutes\n",
      " - PPO Validation from:  20180103 to  20180405\n",
      " - PPO Sharpe Ratio:  0.013035310674267171\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4885287046432495  minutes\n",
      " - DDPG Validation from:  20180103 to  20180405\n",
      " - Trading from:  20180405 to  20180705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1344141.9069425096\n",
      "end_total_asset:1296896.23887736\n",
      "total_reward:-47245.66806514957\n",
      "total_cost:  6673.985568705709\n",
      "total trades:  1021\n",
      "Sharpe:  -0.2109206937691481\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180405\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.035525878270467  minutes\n",
      " - A2C Validation from:  20180405 to  20180705\n",
      " - A2C Sharpe Ratio:  -0.1603591868028151\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.797957781950633  minutes\n",
      " - PPO Validation from:  20180405 to  20180705\n",
      " - PPO Sharpe Ratio:  -0.14277755233095316\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.48177566528320315  minutes\n",
      " - DDPG Validation from:  20180405 to  20180705\n",
      " - Trading from:  20180705 to  20181003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1296896.23887736\n",
      "end_total_asset:1311247.4616656264\n",
      "total_reward:14351.222788266372\n",
      "total_cost:  4442.075610085968\n",
      "total trades:  503\n",
      "Sharpe:  0.18358559691317444\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180705\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9279716928799947  minutes\n",
      " - A2C Validation from:  20180705 to  20181003\n",
      " - A2C Sharpe Ratio:  0.1589314848102178\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.4358166019121805  minutes\n",
      " - PPO Validation from:  20180705 to  20181003\n",
      " - PPO Sharpe Ratio:  0.0817447937197655\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.523938798904419  minutes\n",
      " - DDPG Validation from:  20180705 to  20181003\n",
      " - Trading from:  20181003 to  20190104\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1311247.4616656264\n",
      "end_total_asset:1325994.2908630644\n",
      "total_reward:14746.829197437968\n",
      "total_cost:  1097.5477821234929\n",
      "total trades:  135\n",
      "Sharpe:  0.2791529889888916\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20181003\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9642818411191304  minutes\n",
      " - A2C Validation from:  20181003 to  20190104\n",
      " - A2C Sharpe Ratio:  -0.3430979917809828\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.2774181962013245  minutes\n",
      " - PPO Validation from:  20181003 to  20190104\n",
      " - PPO Sharpe Ratio:  -0.4110962106987585\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4913601835568746  minutes\n",
      " - DDPG Validation from:  20181003 to  20190104\n",
      " - Trading from:  20190104 to  20190405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1325994.2908630644\n",
      "end_total_asset:1377105.7415380692\n",
      "total_reward:51111.45067500486\n",
      "total_cost:  5601.218302564972\n",
      "total trades:  1180\n",
      "Sharpe:  0.13847573208983738\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190104\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9400126814842225  minutes\n",
      " - A2C Validation from:  20190104 to  20190405\n",
      " - A2C Sharpe Ratio:  0.13816821481456282\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.249258069197337  minutes\n",
      " - PPO Validation from:  20190104 to  20190405\n",
      " - PPO Sharpe Ratio:  0.07317009940443614\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.49068370660146077  minutes\n",
      " - DDPG Validation from:  20190104 to  20190405\n",
      " - Trading from:  20190405 to  20190708\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1377105.7415380692\n",
      "end_total_asset:1381530.2222050587\n",
      "total_reward:4424.480666989461\n",
      "total_cost:  1400.2710810588926\n",
      "total trades:  129\n",
      "Sharpe:  0.17889045678022913\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190405\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.9639249483744303  minutes\n",
      " - A2C Validation from:  20190405 to  20190708\n",
      " - A2C Sharpe Ratio:  0.5257950613767768\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.300584165255229  minutes\n",
      " - PPO Validation from:  20190405 to  20190708\n",
      " - PPO Sharpe Ratio:  0.260558888092316\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.5295516649881998  minutes\n",
      " - DDPG Validation from:  20190405 to  20190708\n",
      " - Trading from:  20190708 to  20191004\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1381530.2222050587\n",
      "end_total_asset:1392759.1856597045\n",
      "total_reward:11228.963454645826\n",
      "total_cost:  2078.305762335421\n",
      "total trades:  321\n",
      "Sharpe:  0.11870346449264361\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190708\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.0151594122250875  minutes\n",
      " - A2C Validation from:  20190708 to  20191004\n",
      " - A2C Sharpe Ratio:  -0.10676179268624542\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.3464426676432293  minutes\n",
      " - PPO Validation from:  20190708 to  20191004\n",
      " - PPO Sharpe Ratio:  -0.027435980182583607\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.5017219742139181  minutes\n",
      " - DDPG Validation from:  20190708 to  20191004\n",
      " - Trading from:  20191004 to  20200106\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1392759.1856597045\n",
      "end_total_asset:1391903.1043775436\n",
      "total_reward:-856.0812821609434\n",
      "total_cost:  513.8941620693207\n",
      "total trades:  68\n",
      "Sharpe:  -0.3407583330798236\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20191004\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.0062557419141134  minutes\n",
      " - A2C Validation from:  20191004 to  20200106\n",
      " - A2C Sharpe Ratio:  -0.35751065897380285\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.4010780572891237  minutes\n",
      " - PPO Validation from:  20191004 to  20200106\n",
      " - PPO Sharpe Ratio:  -0.03492448849528441\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4907380104064941  minutes\n",
      " - DDPG Validation from:  20191004 to  20200106\n",
      " - Trading from:  20200106 to  20200406\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1391903.1043775436\n",
      "end_total_asset:1379724.3746251082\n",
      "total_reward:-12178.729752435349\n",
      "total_cost:  786.5165627388785\n",
      "total trades:  156\n",
      "Sharpe:  -0.39172543921489905\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20200106\n",
      " - A2C Training\n",
      " - Training time (A2C):  0.956163247426351  minutes\n",
      " - A2C Validation from:  20200106 to  20200406\n",
      " - A2C Sharpe Ratio:  -0.40513692876552143\n",
      " - PPO Training\n",
      " - Training time (PPO):  3.4496540784835816  minutes\n",
      " - PPO Validation from:  20200106 to  20200406\n",
      " - PPO Sharpe Ratio:  -0.39407924927261495\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  0.4959988753000895  minutes\n",
      " - DDPG Validation from:  20200106 to  20200406\n",
      " - Trading from:  20200406 to  20200707\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1379724.3746251082\n",
      "end_total_asset:1384755.0960574269\n",
      "total_reward:5030.721432318678\n",
      "total_cost:  595.6346508163069\n",
      "total trades:  101\n",
      "Sharpe:  0.270313728965662\n",
      "Ensemble Strategy took:  88.8871068318685  minutes\n"
     ]
    }
   ],
   "source": [
    "run_ensemble_strategy(df=df, \n",
    "                      unique_trade_date= unique_trade_date,\n",
    "                      rebalance_window = rebalance_window,\n",
    "                      validation_window=validation_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-sense",
   "metadata": {
    "papermill": {
     "duration": 0.074992,
     "end_time": "2021-04-03T07:46:32.497986",
     "exception": false,
     "start_time": "2021-04-03T07:46:32.422994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"references\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>References\n",
    "        <a class=\"anchor-link\" href=\"#references\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-puzzle",
   "metadata": {
    "papermill": {
     "duration": 0.074825,
     "end_time": "2021-04-03T07:46:32.648223",
     "exception": false,
     "start_time": "2021-04-03T07:46:32.573398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy.<br>\n",
    "In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10126.704499,
   "end_time": "2021-04-03T07:46:34.243540",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-03T04:57:47.539041",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
