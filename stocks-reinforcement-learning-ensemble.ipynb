{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d74b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c99581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "\n",
    "class StockEnvTrain(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False             \n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.cost = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        self.trades = 0\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.state[index+STOCK_DIM+1] > 0:\n",
    "            #update balance\n",
    "            self.state[0] += \\\n",
    "            self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             (1- TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "             TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        available_amount = self.state[0] // self.state[index+1]\n",
    "        # print('available_amount:{}'.format(available_amount))\n",
    "\n",
    "        #update balance\n",
    "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                          (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "        self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "\n",
    "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                          TRANSACTION_FEE_PERCENT\n",
    "        self.trades+=1\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        # print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('./working/account_value_train.png') # get rid of leading '\\'\n",
    "            plt.close()\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            \n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('./working/account_value_train.csv')\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total_trades: \", self.trades)\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            #print(\"Sharpe: \",sharpe)\n",
    "            #print(\"=================================\")\n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('/kaggle./working/account_rewards_train.csv')\n",
    "            \n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #actions = (actions.astype(int))\n",
    "            \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "        # iteration += 1 \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ff05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "\n",
    "# turbulence index: 90-150 reasonable threshold\n",
    "#TURBULENCE_THRESHOLD = 140\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvTrade(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df,day = 0,turbulence_threshold=140\n",
    "                 ,initial=True, previous_state=[], model_name='', iteration=''):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "        self.model_name=model_name        \n",
    "        self.iteration=iteration\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence<self.turbulence_threshold:\n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += \\\n",
    "                self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 (1- TRANSACTION_FEE_PERCENT)\n",
    "                \n",
    "                self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just clear out all positions \n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              (1- TRANSACTION_FEE_PERCENT)\n",
    "                self.state[index+STOCK_DIM+1] =0\n",
    "                self.cost += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence< self.turbulence_threshold:\n",
    "            available_amount = self.state[0] // self.state[index+1]\n",
    "            # print('available_amount:{}'.format(available_amount))\n",
    "            \n",
    "            #update balance\n",
    "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                              (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "            \n",
    "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just stop buying\n",
    "            pass\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        # print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('./working/account_value_trade_{}_{}.png'.format(self.model_name, self.iteration))\n",
    "            plt.close()\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('./working/account_value_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "\n",
    "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))- self.asset_memory[0] ))\n",
    "            print(\"total_cost: \", self.cost)\n",
    "            print(\"total trades: \", self.trades)\n",
    "\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            print(\"Sharpe: \",sharpe)\n",
    "            \n",
    "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            df_rewards.to_csv('./working/account_rewards_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
    "            \n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #actions = (actions.astype(int))\n",
    "            if self.turbulence>=self.turbulence_threshold:\n",
    "                actions=np.array([-HMAX_NORMALIZE]*STOCK_DIM)\n",
    "                \n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            self.turbulence = self.data['turbulence'].values[0]\n",
    "            #print(self.turbulence)\n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):  \n",
    "        if self.initial:\n",
    "            self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "            self.day = 0\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.turbulence = 0\n",
    "            self.cost = 0\n",
    "            self.trades = 0\n",
    "            self.terminal = False \n",
    "            #self.iteration=self.iteration\n",
    "            self.rewards_memory = []\n",
    "            #initiate state\n",
    "            self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                          self.data.adjcp.values.tolist() + \\\n",
    "                          [0]*STOCK_DIM + \\\n",
    "                          self.data.macd.values.tolist() + \\\n",
    "                          self.data.rsi.values.tolist()  + \\\n",
    "                          self.data.cci.values.tolist()  + \\\n",
    "                          self.data.adx.values.tolist() \n",
    "        else:\n",
    "            previous_total_asset = self.previous_state[0]+ \\\n",
    "            sum(np.array(self.previous_state[1:(STOCK_DIM+1)])*np.array(self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "            #self.asset_memory = [self.previous_state[0]]\n",
    "            self.day = 0\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.turbulence = 0\n",
    "            self.cost = 0\n",
    "            self.trades = 0\n",
    "            self.terminal = False \n",
    "            #self.iteration=iteration\n",
    "            self.rewards_memory = []\n",
    "            #initiate state\n",
    "            #self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]\n",
    "            #[0]*STOCK_DIM + \\\n",
    "\n",
    "            self.state = [ self.previous_state[0]] + \\\n",
    "                          self.data.adjcp.values.tolist() + \\\n",
    "                          self.previous_state[(STOCK_DIM+1):(STOCK_DIM*2+1)]+ \\\n",
    "                          self.data.macd.values.tolist() + \\\n",
    "                          self.data.rsi.values.tolist()  + \\\n",
    "                          self.data.cci.values.tolist()  + \\\n",
    "                          self.data.adx.values.tolist() \n",
    "            \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eae39b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# shares normalization factor\n",
    "# 100 shares per trade\n",
    "HMAX_NORMALIZE = 100\n",
    "# initial amount of money we have in our account\n",
    "INITIAL_ACCOUNT_BALANCE=1000000\n",
    "# total number of stocks in our portfolio\n",
    "STOCK_DIM = 30\n",
    "# transaction fee: 1/1000 reasonable percentage\n",
    "TRANSACTION_FEE_PERCENT = 0.001\n",
    "\n",
    "# turbulence index: 90-150 reasonable threshold\n",
    "#TURBULENCE_THRESHOLD = 140\n",
    "REWARD_SCALING = 1e-4\n",
    "\n",
    "class StockEnvValidation(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, day = 0, turbulence_threshold=140, iteration=''):\n",
    "        #super(StockEnv, self).__init__()\n",
    "        #money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.df = df\n",
    "        # action_space normalization and shape is STOCK_DIM\n",
    "        self.action_space = spaces.Box(low = -1, high = 1,shape = (STOCK_DIM,)) \n",
    "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
    "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist() + \\\n",
    "                      self.data.cci.values.tolist() + \\\n",
    "                      self.data.adx.values.tolist()\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.rewards_memory = []\n",
    "        #self.reset()\n",
    "        self._seed()\n",
    "        \n",
    "        self.iteration=iteration\n",
    "\n",
    "\n",
    "    def _sell_stock(self, index, action):\n",
    "        # perform sell action based on the sign of the action\n",
    "        if self.turbulence<self.turbulence_threshold:\n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += \\\n",
    "                self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 (1- TRANSACTION_FEE_PERCENT)\n",
    "                \n",
    "                self.state[index+STOCK_DIM+1] -= min(abs(action), self.state[index+STOCK_DIM+1])\n",
    "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+STOCK_DIM+1]) * \\\n",
    "                 TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just clear out all positions \n",
    "            if self.state[index+STOCK_DIM+1] > 0:\n",
    "                #update balance\n",
    "                self.state[0] += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              (1- TRANSACTION_FEE_PERCENT)\n",
    "                self.state[index+STOCK_DIM+1] =0\n",
    "                self.cost += self.state[index+1]*self.state[index+STOCK_DIM+1]* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "                self.trades+=1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def _buy_stock(self, index, action):\n",
    "        # perform buy action based on the sign of the action\n",
    "        if self.turbulence< self.turbulence_threshold:\n",
    "            available_amount = self.state[0] // self.state[index+1]\n",
    "            # print('available_amount:{}'.format(available_amount))\n",
    "            \n",
    "            #update balance\n",
    "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
    "                              (1+ TRANSACTION_FEE_PERCENT)\n",
    "\n",
    "            self.state[index+STOCK_DIM+1] += min(available_amount, action)\n",
    "            \n",
    "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
    "                              TRANSACTION_FEE_PERCENT\n",
    "            self.trades+=1\n",
    "        else:\n",
    "            # if turbulence goes over threshold, just stop buying\n",
    "            pass\n",
    "        \n",
    "    def step(self, actions):\n",
    "        # print(self.day)\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        # print(actions)\n",
    "\n",
    "        if self.terminal:\n",
    "            plt.plot(self.asset_memory,'r')\n",
    "            plt.savefig('./working/account_value_validation_{}.png'.format(self.iteration))\n",
    "            plt.close()\n",
    "            df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            df_total_value.to_csv('./working/account_value_validation_{}.csv'.format(self.iteration))\n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):61]))- self.asset_memory[0] ))\n",
    "            #print(\"total_cost: \", self.cost)\n",
    "            #print(\"total trades: \", self.trades)\n",
    "\n",
    "            df_total_value.columns = ['account_value']\n",
    "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
    "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
    "                  df_total_value['daily_return'].std()\n",
    "            #print(\"Sharpe: \",sharpe)\n",
    "            \n",
    "            #df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            #df_rewards.to_csv('/kaggle./working/account_rewards_trade_{}.csv'.format(self.iteration))\n",
    "            \n",
    "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
    "            #with open('obs.pkl', 'wb') as f:  \n",
    "            #    pickle.dump(self.state, f)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        else:\n",
    "            # print(np.array(self.state[1:29]))\n",
    "\n",
    "            actions = actions * HMAX_NORMALIZE\n",
    "            #actions = (actions.astype(int))\n",
    "            if self.turbulence>=self.turbulence_threshold:\n",
    "                actions=np.array([-HMAX_NORMALIZE]*STOCK_DIM)\n",
    "            begin_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
    "            \n",
    "            argsort_actions = np.argsort(actions)\n",
    "            \n",
    "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
    "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
    "\n",
    "            for index in sell_index:\n",
    "                # print('take sell action'.format(actions[index]))\n",
    "                self._sell_stock(index, actions[index])\n",
    "\n",
    "            for index in buy_index:\n",
    "                # print('take buy action: {}'.format(actions[index]))\n",
    "                self._buy_stock(index, actions[index])\n",
    "\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]         \n",
    "            self.turbulence = self.data['turbulence'].values[0]\n",
    "            #print(self.turbulence)\n",
    "            #load next state\n",
    "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
    "            self.state =  [self.state[0]] + \\\n",
    "                    self.data.adjcp.values.tolist() + \\\n",
    "                    list(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]) + \\\n",
    "                    self.data.macd.values.tolist() + \\\n",
    "                    self.data.rsi.values.tolist() + \\\n",
    "                    self.data.cci.values.tolist() + \\\n",
    "                    self.data.adx.values.tolist()\n",
    "            \n",
    "            end_total_asset = self.state[0]+ \\\n",
    "            sum(np.array(self.state[1:(STOCK_DIM+1)])*np.array(self.state[(STOCK_DIM+1):(STOCK_DIM*2+1)]))\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
    "            \n",
    "            self.reward = end_total_asset - begin_total_asset            \n",
    "            # print(\"step_reward:{}\".format(self.reward))\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            \n",
    "            self.reward = self.reward*REWARD_SCALING\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):  \n",
    "        self.asset_memory = [INITIAL_ACCOUNT_BALANCE]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False \n",
    "        #self.iteration=self.iteration\n",
    "        self.rewards_memory = []\n",
    "        #initiate state\n",
    "        self.state = [INITIAL_ACCOUNT_BALANCE] + \\\n",
    "                      self.data.adjcp.values.tolist() + \\\n",
    "                      [0]*STOCK_DIM + \\\n",
    "                      self.data.macd.values.tolist() + \\\n",
    "                      self.data.rsi.values.tolist()  + \\\n",
    "                      self.data.cci.values.tolist()  + \\\n",
    "                      self.data.adx.values.tolist() \n",
    "            \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human',close=False):\n",
    "        return self.state\n",
    "    \n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smooth-clothing",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-04-03T04:57:54.198152Z",
     "iopub.status.busy": "2021-04-03T04:57:54.197005Z",
     "iopub.status.idle": "2021-04-03T05:00:42.175245Z",
     "shell.execute_reply": "2021-04-03T05:00:42.173533Z"
    },
    "papermill": {
     "duration": 167.993805,
     "end_time": "2021-04-03T05:00:42.175556",
     "exception": false,
     "start_time": "2021-04-03T04:57:54.181751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#from env.EnvMultipleStock_train import StockEnvTrain\n",
    "#from env.EnvMultipleStock_validation import StockEnvValidation\n",
    "#from env.EnvMultipleStock_trade import StockEnvTrade\n",
    "\n",
    "#!pip uninstall -y tensorflow-probability\n",
    "#!pip uninstall -y tensorflow-cloud\n",
    "#!pip uninstall -y pytorch-lightning\n",
    "#!pip uninstall -y tensorflow\n",
    "#!pip uninstall -y gast\n",
    "\n",
    "#!pip install -qq 'tensorflow==1.15.0'\n",
    "import tensorflow as tf\n",
    "\n",
    "#!apt-get update > /dev/null\n",
    "#!apt-get install -qq -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
    "#!pip install -qq \"stable-baselines[mpi]==2.9.0\"\n",
    "\n",
    "from stable_baselines import GAIL, SAC\n",
    "from stable_baselines import ACER\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines import DDPG\n",
    "from stable_baselines import TD3\n",
    "from stable_baselines import DQN\n",
    "\n",
    "from stable_baselines.ddpg.policies import DDPGPolicy\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\n",
    "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec\n",
    "from stable_baselines.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-calcium",
   "metadata": {
    "papermill": {
     "duration": 0.035776,
     "end_time": "2021-04-03T05:00:42.246099",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.210323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"dataset\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Dataset\n",
    "        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "registered-specialist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:42.319007Z",
     "iopub.status.busy": "2021-04-03T05:00:42.318274Z",
     "iopub.status.idle": "2021-04-03T05:00:42.766545Z",
     "shell.execute_reply": "2021-04-03T05:00:42.765961Z"
    },
    "papermill": {
     "duration": 0.486202,
     "end_time": "2021-04-03T05:00:42.766718",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.280516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>datadate</th>\n",
       "      <th>tic</th>\n",
       "      <th>adjcp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>cci</th>\n",
       "      <th>adx</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20090102</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>12.964286</td>\n",
       "      <td>12.268571</td>\n",
       "      <td>13.005714</td>\n",
       "      <td>12.165714</td>\n",
       "      <td>26641980.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20090102</td>\n",
       "      <td>AXP</td>\n",
       "      <td>19.330000</td>\n",
       "      <td>18.570000</td>\n",
       "      <td>19.520000</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>10955620.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>20090102</td>\n",
       "      <td>BA</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>42.800000</td>\n",
       "      <td>45.560000</td>\n",
       "      <td>42.780000</td>\n",
       "      <td>7010171.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>20090102</td>\n",
       "      <td>CAT</td>\n",
       "      <td>46.910000</td>\n",
       "      <td>44.910000</td>\n",
       "      <td>46.980000</td>\n",
       "      <td>44.710000</td>\n",
       "      <td>7116726.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20090102</td>\n",
       "      <td>CSCO</td>\n",
       "      <td>16.960000</td>\n",
       "      <td>16.410000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>40977480.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  datadate   tic      adjcp       open       high        low  \\\n",
       "0           0  20090102  AAPL  12.964286  12.268571  13.005714  12.165714   \n",
       "1           1  20090102   AXP  19.330000  18.570000  19.520000  18.400000   \n",
       "2           2  20090102    BA  45.250000  42.800000  45.560000  42.780000   \n",
       "3           3  20090102   CAT  46.910000  44.910000  46.980000  44.710000   \n",
       "4           4  20090102  CSCO  16.960000  16.410000  17.000000  16.250000   \n",
       "\n",
       "       volume  macd    rsi        cci    adx  turbulence  \n",
       "0  26641980.0   0.0  100.0  66.666667  100.0         0.0  \n",
       "1  10955620.0   0.0  100.0  66.666667  100.0         0.0  \n",
       "2   7010171.0   0.0  100.0  66.666667  100.0         0.0  \n",
       "3   7116726.0   0.0    0.0  66.666667  100.0         0.0  \n",
       "4  40977480.0   0.0  100.0  66.666667  100.0         0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path = '/kaggle/input/trading/trading.csv'\n",
    "path = 'trading.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "understanding-fields",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:42.841646Z",
     "iopub.status.busy": "2021-04-03T05:00:42.841006Z",
     "iopub.status.idle": "2021-04-03T05:00:42.845211Z",
     "shell.execute_reply": "2021-04-03T05:00:42.844583Z"
    },
    "papermill": {
     "duration": 0.043923,
     "end_time": "2021-04-03T05:00:42.845383",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.801460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63\n",
    "validation_window = 63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cheap-madison",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:42.927058Z",
     "iopub.status.busy": "2021-04-03T05:00:42.925790Z",
     "iopub.status.idle": "2021-04-03T05:00:42.938471Z",
     "shell.execute_reply": "2021-04-03T05:00:42.937753Z"
    },
    "papermill": {
     "duration": 0.055925,
     "end_time": "2021-04-03T05:00:42.938636",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.882711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20151002 20151005 20151006 ... 20200702 20200706 20200707]\n"
     ]
    }
   ],
   "source": [
    "unique_trade_date = df[(df.datadate > 20151001)&(df.datadate <= 20200707)].datadate.unique()\n",
    "print(unique_trade_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-lease",
   "metadata": {
    "papermill": {
     "duration": 0.036451,
     "end_time": "2021-04-03T05:00:43.010626",
     "exception": false,
     "start_time": "2021-04-03T05:00:42.974175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"stable\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Stable Baseline\n",
    "        <a class=\"anchor-link\" href=\"#stable\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "senior-dispatch",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.101017Z",
     "iopub.status.busy": "2021-04-03T05:00:43.100132Z",
     "iopub.status.idle": "2021-04-03T05:00:43.103435Z",
     "shell.execute_reply": "2021-04-03T05:00:43.102911Z"
    },
    "papermill": {
     "duration": 0.056248,
     "end_time": "2021-04-03T05:00:43.103598",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.047350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_A2C(env_train, model_name, timesteps=25000):\n",
    "    start = time.time()\n",
    "    model = A2C('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (A2C): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_ACER(env_train, model_name, timesteps=25000):\n",
    "    start = time.time()\n",
    "    model = ACER('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (A2C): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_DDPG(env_train, model_name, timesteps=10000):\n",
    "    # add the noise objects for DDPG\n",
    "    n_actions = env_train.action_space.shape[-1]\n",
    "    param_noise = None\n",
    "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
    "\n",
    "    start = time.time()\n",
    "    model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (DDPG): ', (end-start)/60,' minutes')\n",
    "    return model\n",
    "\n",
    "def train_PPO(env_train, model_name, timesteps=50000):\n",
    "    start = time.time()\n",
    "    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_GAIL(env_train, model_name, timesteps=1000):\n",
    "    start = time.time()\n",
    "    # generate expert trajectories\n",
    "    model = SAC('MLpPolicy', env_train, verbose=1)\n",
    "    GAIL.generate_expert_traj(model, 'expert_model_gail', n_timesteps=100, n_episodes=10)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = GAIL.ExpertDataset(expert_path='expert_model_gail.npz', traj_limitation=10, verbose=1)\n",
    "    model = GAIL('MLpPolicy', env_train, dataset, verbose=1)\n",
    "\n",
    "    model.learn(total_timesteps=1000)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (PPO): ', (end - start) / 60, ' minutes')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b2338",
   "metadata": {},
   "source": [
    "##### Our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b4ff1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DQN(env_train, model_name, timesteps=25000):\n",
    "    start = time.time()\n",
    "    model = DQN('MlpPolicy', env_train, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (DQN): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_TD3(env_train, model_name, timesteps=25000):\n",
    "    n_actions = env_train.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    \n",
    "    start = time.time()\n",
    "    model = TD3('MlpPolicy', env_train, action_noise=action_noise, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (TD3): ', (end - start) / 60, ' minutes')\n",
    "    return model\n",
    "\n",
    "def train_SAC(env_train, model_name, timesteps=25000):\n",
    "    n_actions = env_train.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "    start = time.time()\n",
    "    model = SAC('MlpPolicy', env_train, action_noise=action_noise, verbose=0)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    end = time.time()\n",
    "\n",
    "    model.save(f\"./working/{model_name}\")\n",
    "    print(' - Training time (SAC): ', (end - start) / 60, ' minutes')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-camera",
   "metadata": {
    "papermill": {
     "duration": 0.035164,
     "end_time": "2021-04-03T05:00:43.174458",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.139294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"additional\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Additional Functions\n",
    "        <a class=\"anchor-link\" href=\"#additional\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "complex-berkeley",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.254427Z",
     "iopub.status.busy": "2021-04-03T05:00:43.253753Z",
     "iopub.status.idle": "2021-04-03T05:00:43.257932Z",
     "shell.execute_reply": "2021-04-03T05:00:43.257250Z"
    },
    "papermill": {
     "duration": 0.048232,
     "end_time": "2021-04-03T05:00:43.258157",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.209925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_split(df,start,end):\n",
    "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
    "    data=data.sort_values(['datadate','tic'],ignore_index=True)\n",
    "    data.index = data.datadate.factorize()[0]\n",
    "    return data\n",
    "\n",
    "def get_validation_sharpe(iteration):\n",
    "    df_total_value = pd.read_csv('./working/account_value_validation_{}.csv'.format(iteration), index_col=0)\n",
    "    df_total_value.columns = ['account_value_train']\n",
    "    df_total_value['daily_return'] = df_total_value.pct_change(1)\n",
    "    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\n",
    "             df_total_value['daily_return'].std()\n",
    "    return sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-sunrise",
   "metadata": {
    "papermill": {
     "duration": 0.034922,
     "end_time": "2021-04-03T05:00:43.328501",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.293579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"predvalid\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Predict-Validate\n",
    "        <a class=\"anchor-link\" href=\"#predvalid\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stock-robert",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.411283Z",
     "iopub.status.busy": "2021-04-03T05:00:43.410568Z",
     "iopub.status.idle": "2021-04-03T05:00:43.414371Z",
     "shell.execute_reply": "2021-04-03T05:00:43.413801Z"
    },
    "papermill": {
     "duration": 0.05106,
     "end_time": "2021-04-03T05:00:43.414516",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.363456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DRL_prediction(df,\n",
    "                   model,\n",
    "                   name,\n",
    "                   last_state,\n",
    "                   iter_num,\n",
    "                   unique_trade_date,\n",
    "                   rebalance_window,\n",
    "                   turbulence_threshold,\n",
    "                   initial):\n",
    "\n",
    "    trade_data = data_split(df, start=unique_trade_date[iter_num - rebalance_window], end=unique_trade_date[iter_num])\n",
    "    env_trade = DummyVecEnv([lambda: StockEnvTrade(trade_data,\n",
    "                                                   turbulence_threshold=turbulence_threshold,\n",
    "                                                   initial=initial,\n",
    "                                                   previous_state=last_state,\n",
    "                                                   model_name=name,\n",
    "                                                   iteration=iter_num)])\n",
    "    obs_trade = env_trade.reset()\n",
    "\n",
    "    for i in range(len(trade_data.index.unique())):\n",
    "        action, _states = model.predict(obs_trade)\n",
    "        obs_trade, rewards, dones, info = env_trade.step(action)\n",
    "        if i == (len(trade_data.index.unique()) - 2):\n",
    "            last_state = env_trade.render()\n",
    "\n",
    "    df_last_state = pd.DataFrame({'last_state': last_state})\n",
    "    df_last_state.to_csv('./working/last_state_{}_{}.csv'.format(name, i), index=False)\n",
    "    return last_state\n",
    "\n",
    "def DRL_validation(model, test_data, test_env, test_obs) -> None:\n",
    "    for i in range(len(test_data.index.unique())):\n",
    "        action, _states = model.predict(test_obs)\n",
    "        test_obs, rewards, dones, info = test_env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-anthropology",
   "metadata": {
    "papermill": {
     "duration": 0.036259,
     "end_time": "2021-04-03T05:00:43.486375",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.450116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"ensemble\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>Ensemble\n",
    "        <a class=\"anchor-link\" href=\"#ensemble\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrapped-pathology",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.582758Z",
     "iopub.status.busy": "2021-04-03T05:00:43.581780Z",
     "iopub.status.idle": "2021-04-03T05:00:43.585359Z",
     "shell.execute_reply": "2021-04-03T05:00:43.584799Z"
    },
    "papermill": {
     "duration": 0.063935,
     "end_time": "2021-04-03T05:00:43.585499",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.521564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_ensemble_strategy(df, unique_trade_date, rebalance_window, validation_window) -> None:\n",
    "    last_state_ensemble = []\n",
    "    ppo_sharpe_list = []\n",
    "    ddpg_sharpe_list = []\n",
    "    a2c_sharpe_list = []\n",
    "\n",
    "    model_use = []\n",
    "\n",
    "    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n",
    "    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n",
    "    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n",
    "        if i - rebalance_window - validation_window == 0:\n",
    "            # inital state\n",
    "            initial = True\n",
    "        else:\n",
    "            # previous state\n",
    "            initial = False\n",
    "\n",
    "        # Tuning trubulence index based on historical data\n",
    "        # Turbulence lookback window is one quarter\n",
    "        end_date_index = df.index[df[\"datadate\"] == unique_trade_date[i - rebalance_window - validation_window]].to_list()[-1]\n",
    "        start_date_index = end_date_index - validation_window*30 + 1\n",
    "\n",
    "        historical_turbulence = df.iloc[start_date_index:(end_date_index + 1), :]\n",
    "        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n",
    "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)\n",
    "\n",
    "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
    "            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n",
    "            # then we assume that the current market is volatile,\n",
    "            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold\n",
    "            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n",
    "            turbulence_threshold = insample_turbulence_threshold\n",
    "        else:\n",
    "            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n",
    "            # then we tune up the turbulence_threshold, meaning we lower the risk\n",
    "            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n",
    "            \n",
    "        print(\"-\" * 50)\n",
    "        print(\" - Turbulence_threshold: \", turbulence_threshold)\n",
    "\n",
    "        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n",
    "        env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "\n",
    "        ## validation env\n",
    "        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n",
    "                                end=unique_trade_date[i - rebalance_window])\n",
    "        env_val = DummyVecEnv([lambda: StockEnvValidation(validation,\n",
    "                                                          turbulence_threshold=turbulence_threshold,\n",
    "                                                          iteration=i)])\n",
    "        obs_val = env_val.reset()\n",
    "        \n",
    "        print(\" - Model training from: \", 20090000, \"to \",\n",
    "              unique_trade_date[i - rebalance_window - validation_window])\n",
    "        print(\" - A2C Training\")\n",
    "        model_a2c = train_A2C(env_train, model_name=\"A2C_30k_dow_{}\".format(i), timesteps=30000)\n",
    "        print(\" - A2C Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_a2c, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_a2c = get_validation_sharpe(i)\n",
    "        print(\" - A2C Sharpe Ratio: \", sharpe_a2c)\n",
    "\n",
    "        print(\" - PPO Training\")\n",
    "        model_ppo = train_PPO(env_train, model_name=\"PPO_100k_dow_{}\".format(i), timesteps=100000)\n",
    "        print(\" - PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ppo = get_validation_sharpe(i)\n",
    "        print(\" - PPO Sharpe Ratio: \", sharpe_ppo)\n",
    "\n",
    "        print(\" - DDPG Training\")\n",
    "        model_ddpg = train_DDPG(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=10000)\n",
    "        print(\" - DDPG Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        \n",
    "        DRL_validation(model=model_ddpg, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ddpg = get_validation_sharpe(i)\n",
    "\n",
    "        ppo_sharpe_list.append(sharpe_ppo)\n",
    "        a2c_sharpe_list.append(sharpe_a2c)\n",
    "        ddpg_sharpe_list.append(sharpe_ddpg)\n",
    "\n",
    "        # Model Selection based on sharpe ratio\n",
    "        if (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n",
    "            model_ensemble = model_ppo\n",
    "            model_use.append('PPO')\n",
    "        elif (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n",
    "            model_ensemble = model_a2c\n",
    "            model_use.append('A2C')\n",
    "        else:\n",
    "            model_ensemble = model_ddpg\n",
    "            model_use.append('DDPG')\n",
    "\n",
    "        print(\" - Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n",
    "        print(\"-\" * 50)\n",
    "        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n",
    "                                             last_state=last_state_ensemble, iter_num=i,\n",
    "                                             unique_trade_date=unique_trade_date,\n",
    "                                             rebalance_window=rebalance_window,\n",
    "                                             turbulence_threshold=turbulence_threshold,\n",
    "                                             initial=initial)\n",
    "        \n",
    "    end = time.time()\n",
    "    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a82150",
   "metadata": {},
   "source": [
    "##### Our Ensemble Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d23c405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble_strategy_new(df, unique_trade_date, rebalance_window, validation_window) -> None:\n",
    "    last_state_ensemble = []\n",
    "    ppo_sharpe_list = []\n",
    "    sac_sharpe_list = []\n",
    "    td3_sharpe_list = []\n",
    "    \n",
    "    model_use = []\n",
    "    \n",
    "    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n",
    "    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n",
    "    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n",
    "        if i - rebalance_window - validation_window == 0:\n",
    "            initial = True\n",
    "        else:\n",
    "            initial = False\n",
    "\n",
    "        end_date_index = df.index[df[\"datadate\"] == unique_trade_date[i - rebalance_window - validation_window]].to_list()[-1]\n",
    "        start_date_index = end_date_index - validation_window * 30 + 1\n",
    "\n",
    "        historical_turbulence = df.iloc[start_date_index:(end_date_index + 1), :]\n",
    "        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n",
    "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)\n",
    "\n",
    "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
    "            turbulence_threshold = insample_turbulence_threshold\n",
    "        else:\n",
    "            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        print(\" - Turbulence_threshold: \", turbulence_threshold)\n",
    "\n",
    "        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n",
    "        env_train = DummyVecEnv([lambda: StockEnvTrain(train)])\n",
    "\n",
    "        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n",
    "                                end=unique_trade_date[i - rebalance_window])\n",
    "        env_val = DummyVecEnv([lambda: StockEnvValidation(validation,\n",
    "                                                          turbulence_threshold=turbulence_threshold,\n",
    "                                                          iteration=i)])\n",
    "        obs_val = env_val.reset()\n",
    "        \n",
    "        print(\" - Model training from: \", 20090000, \"to \", unique_trade_date[i - rebalance_window - validation_window])\n",
    "\n",
    "        # PROBLEM: DQN NEEDS DISCRETE ACTION SPACE -> ERROR WHEN USING GIVEN ENVS\n",
    "        # Error: see error.txt\n",
    "       \n",
    "        print(\" - PPO Training\")\n",
    "        model_ppo = train_PPO(env_train, model_name=\"PPO_25k_dow_{}\".format(i), timesteps=100000)\n",
    "        print(\" - PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_ppo = get_validation_sharpe(i)\n",
    "        print(\" - PPO Sharpe Ratio: \", sharpe_ppo)\n",
    "        \n",
    "\n",
    "        # MAY NEED TO PLAY WITH TIMESTEPS DEPENDING ON PERFORMANCE\n",
    "        print(\" - SAC Training\")\n",
    "        model_sac = train_SAC(env_train, model_name=\"SAC_25k_dow_{}\".format(i), timesteps=35000)\n",
    "        print(\" - SAC Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_sac, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_sac = get_validation_sharpe(i)\n",
    "        print(\" - SAC Sharpe Ratio: \", sharpe_sac)\n",
    "\n",
    "        print(\" - TD3 Training\")\n",
    "        model_td3 = train_TD3(env_train, model_name=\"TD3_25k_dow_{}\".format(i), timesteps=15000)\n",
    "        print(\" - TD3 Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
    "              unique_trade_date[i - rebalance_window])\n",
    "        DRL_validation(model=model_td3, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
    "        sharpe_td3 = get_validation_sharpe(i)\n",
    "        print(\" - TD3 Sharpe Ratio: \", sharpe_td3)\n",
    "\n",
    "        # dqn_sharpe_list.append(sharpe_dqn)\n",
    "        sac_sharpe_list.append(sharpe_sac)\n",
    "        td3_sharpe_list.append(sharpe_td3)\n",
    "        ppo_sharpe_list.append(sharpe_ppo)\n",
    "\n",
    "        # NEED TO FIX DQN STUFF\n",
    "    \n",
    "        if (sharpe_ppo >= sharpe_sac) & (sharpe_ppo >= sharpe_td3):\n",
    "            model_ensemble = model_ppo\n",
    "            model_use.append('PPO')\n",
    "        elif (sharpe_sac > sharpe_ppo) & (sharpe_sac > sharpe_td3):\n",
    "            model_ensemble = model_sac\n",
    "            model_use.append('SAC')\n",
    "        else:\n",
    "            model_ensemble = model_td3\n",
    "            model_use.append('TD3')\n",
    "        '''\n",
    "        if (sharpe_sac >= sharpe_td3):\n",
    "            model_ensemble = model_sac\n",
    "            model_use.append('SAC')\n",
    "        else:\n",
    "            model_ensemble = model_td3\n",
    "            model_use.append('TD3')\n",
    "        '''\n",
    "\n",
    "        print(\" - Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n",
    "        print(\"-\" * 50)\n",
    "        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n",
    "                                             last_state=last_state_ensemble, iter_num=i,\n",
    "                                             unique_trade_date=unique_trade_date,\n",
    "                                             rebalance_window=rebalance_window,\n",
    "                                             turbulence_threshold=turbulence_threshold,\n",
    "                                             initial=initial)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23639922",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20151002\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.985371021429698  minutes\n",
      " - PPO Validation from:  20151002 to  20160104\n",
      " - PPO Sharpe Ratio:  0.031157763712205248\n",
      " - SAC Training\n",
      " - Training time (SAC):  55.82115728457769  minutes\n",
      " - SAC Validation from:  20151002 to  20160104\n",
      " - SAC Sharpe Ratio:  -0.02396180452083464\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.5362343907356262  minutes\n",
      " - TD3 Validation from:  20151002 to  20160104\n",
      " - TD3 Sharpe Ratio:  0.013083692099442762\n",
      " - Trading from:  20160104 to  20160405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1000000\n",
      "end_total_asset:1125755.507163652\n",
      "total_reward:125755.50716365199\n",
      "total_cost:  5582.106993201374\n",
      "total trades:  1409\n",
      "Sharpe:  0.4195057176379845\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20160104\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.761601026852926  minutes\n",
      " - PPO Validation from:  20160104 to  20160405\n",
      " - PPO Sharpe Ratio:  0.06229106978528059\n",
      " - SAC Training\n",
      " - Training time (SAC):  3.859985017776489  minutes\n",
      " - SAC Validation from:  20160104 to  20160405\n",
      " - SAC Sharpe Ratio:  0.05785578673220459\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.4095438758532206  minutes\n",
      " - TD3 Validation from:  20160104 to  20160405\n",
      " - TD3 Sharpe Ratio:  0.04629735040687533\n",
      " - Trading from:  20160405 to  20160705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1125755.507163652\n",
      "end_total_asset:1159427.1010229785\n",
      "total_reward:33671.59385932656\n",
      "total_cost:  3511.8836662433314\n",
      "total trades:  1294\n",
      "Sharpe:  0.10806461476757509\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20160405\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.894766231377919  minutes\n",
      " - PPO Validation from:  20160405 to  20160705\n",
      " - PPO Sharpe Ratio:  0.016585768386743825\n",
      " - SAC Training\n",
      " - Training time (SAC):  3.918653746445974  minutes\n",
      " - SAC Validation from:  20160405 to  20160705\n",
      " - SAC Sharpe Ratio:  -0.053706567216677284\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.4562376817067464  minutes\n",
      " - TD3 Validation from:  20160405 to  20160705\n",
      " - TD3 Sharpe Ratio:  0.07669807318913696\n",
      " - Trading from:  20160705 to  20161003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1159427.1010229785\n",
      "end_total_asset:1172517.0471939927\n",
      "total_reward:13089.946171014104\n",
      "total_cost:  1300.9760334990724\n",
      "total trades:  1368\n",
      "Sharpe:  0.05691341603549551\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20160705\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.941156323750814  minutes\n",
      " - PPO Validation from:  20160705 to  20161003\n",
      " - PPO Sharpe Ratio:  0.01926604124877548\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.104007728894552  minutes\n",
      " - SAC Validation from:  20160705 to  20161003\n",
      " - SAC Sharpe Ratio:  0.12424272039408751\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.653695805867513  minutes\n",
      " - TD3 Validation from:  20160705 to  20161003\n",
      " - TD3 Sharpe Ratio:  0.022923386069306627\n",
      " - Trading from:  20161003 to  20170103\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1172517.0471939927\n",
      "end_total_asset:1340902.3867693555\n",
      "total_reward:168385.33957536286\n",
      "total_cost:  983.2004247359868\n",
      "total trades:  676\n",
      "Sharpe:  0.6333117864158933\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20161003\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.961563690503438  minutes\n",
      " - PPO Validation from:  20161003 to  20170103\n",
      " - PPO Sharpe Ratio:  0.41813811512817844\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.359810348351797  minutes\n",
      " - SAC Validation from:  20161003 to  20170103\n",
      " - SAC Sharpe Ratio:  0.38476098833881334\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.7532709042231243  minutes\n",
      " - TD3 Validation from:  20161003 to  20170103\n",
      " - TD3 Sharpe Ratio:  0.35232554326660476\n",
      " - Trading from:  20170103 to  20170404\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1340902.3867693555\n",
      "end_total_asset:1337944.4427183077\n",
      "total_reward:-2957.944051047787\n",
      "total_cost:  4168.245287896723\n",
      "total trades:  1282\n",
      "Sharpe:  -0.0061722076673175735\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20170103\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.984105368455251  minutes\n",
      " - PPO Validation from:  20170103 to  20170404\n",
      " - PPO Sharpe Ratio:  0.26751010984434226\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.865254135926564  minutes\n",
      " - SAC Validation from:  20170103 to  20170404\n",
      " - SAC Sharpe Ratio:  0.39739636184077237\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.9824661493301392  minutes\n",
      " - TD3 Validation from:  20170103 to  20170404\n",
      " - TD3 Sharpe Ratio:  0.05052207023921225\n",
      " - Trading from:  20170404 to  20170705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1337944.4427183077\n",
      "end_total_asset:1375070.9048284811\n",
      "total_reward:37126.46211017342\n",
      "total_cost:  4078.2828222904845\n",
      "total trades:  1060\n",
      "Sharpe:  0.2345217730132187\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20170404\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.1813908616701765  minutes\n",
      " - PPO Validation from:  20170404 to  20170705\n",
      " - PPO Sharpe Ratio:  0.22383661364587784\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.4459598382314045  minutes\n",
      " - SAC Validation from:  20170404 to  20170705\n",
      " - SAC Sharpe Ratio:  0.25232956782260435\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.6355344772338867  minutes\n",
      " - TD3 Validation from:  20170404 to  20170705\n",
      " - TD3 Sharpe Ratio:  0.351674098315688\n",
      " - Trading from:  20170705 to  20171003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1375070.9048284811\n",
      "end_total_asset:1418904.7067344785\n",
      "total_reward:43833.80190599733\n",
      "total_cost:  3095.9980862556276\n",
      "total trades:  956\n",
      "Sharpe:  0.2770831925651693\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20170705\n",
      " - PPO Training\n",
      " - Training time (PPO):  12.297543752193452  minutes\n",
      " - PPO Validation from:  20170705 to  20171003\n",
      " - PPO Sharpe Ratio:  0.4388388547125825\n",
      " - SAC Training\n",
      " - Training time (SAC):  5.134227180480957  minutes\n",
      " - SAC Validation from:  20170705 to  20171003\n",
      " - SAC Sharpe Ratio:  0.18856618107668582\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.6131399154663086  minutes\n",
      " - TD3 Validation from:  20170705 to  20171003\n",
      " - TD3 Sharpe Ratio:  0.20296372821152306\n",
      " - Trading from:  20171003 to  20180103\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1418904.7067344785\n",
      "end_total_asset:1576520.299408155\n",
      "total_reward:157615.59267367655\n",
      "total_cost:  6333.009957309642\n",
      "total trades:  1354\n",
      "Sharpe:  0.6755725297337164\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20171003\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.402727063496908  minutes\n",
      " - PPO Validation from:  20171003 to  20180103\n",
      " - PPO Sharpe Ratio:  0.2872697537906185\n",
      " - SAC Training\n",
      " - Training time (SAC):  17.67582358519236  minutes\n",
      " - SAC Validation from:  20171003 to  20180103\n",
      " - SAC Sharpe Ratio:  0.4587438529886509\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.8728626569112141  minutes\n",
      " - TD3 Validation from:  20171003 to  20180103\n",
      " - TD3 Sharpe Ratio:  0.2613198811335678\n",
      " - Trading from:  20180103 to  20180405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1576520.299408155\n",
      "end_total_asset:1604925.3776776122\n",
      "total_reward:28405.07826945721\n",
      "total_cost:  2261.492194716561\n",
      "total trades:  261\n",
      "Sharpe:  0.10205567254180359\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180103\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.051526391506195  minutes\n",
      " - PPO Validation from:  20180103 to  20180405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - PPO Sharpe Ratio:  0.002343078094571861\n",
      " - SAC Training\n",
      " - Training time (SAC):  5.125976483027141  minutes\n",
      " - SAC Validation from:  20180103 to  20180405\n",
      " - SAC Sharpe Ratio:  0.013815147021206876\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.9295905510584512  minutes\n",
      " - TD3 Validation from:  20180103 to  20180405\n",
      " - TD3 Sharpe Ratio:  -0.054806395423255126\n",
      " - Trading from:  20180405 to  20180705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1604925.3776776122\n",
      "end_total_asset:1603542.0904619154\n",
      "total_reward:-1383.2872156968806\n",
      "total_cost:  3834.2127547704763\n",
      "total trades:  685\n",
      "Sharpe:  0.000726433728432446\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180405\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.151057541370392  minutes\n",
      " - PPO Validation from:  20180405 to  20180705\n",
      " - PPO Sharpe Ratio:  -0.16567934352031963\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.409694548447927  minutes\n",
      " - SAC Validation from:  20180405 to  20180705\n",
      " - SAC Sharpe Ratio:  -0.047369261323765094\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.5353545109430948  minutes\n",
      " - TD3 Validation from:  20180405 to  20180705\n",
      " - TD3 Sharpe Ratio:  -0.09598029965710221\n",
      " - Trading from:  20180705 to  20181003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1603542.0904619154\n",
      "end_total_asset:1622693.226801916\n",
      "total_reward:19151.136340000667\n",
      "total_cost:  3830.8719700000033\n",
      "total trades:  420\n",
      "Sharpe:  0.20262657652506214\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180705\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.800477341810862  minutes\n",
      " - PPO Validation from:  20180705 to  20181003\n",
      " - PPO Sharpe Ratio:  -0.027771190249793114\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.309108503659567  minutes\n",
      " - SAC Validation from:  20180705 to  20181003\n",
      " - SAC Sharpe Ratio:  0.16204621695063795\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.6228751301765443  minutes\n",
      " - TD3 Validation from:  20180705 to  20181003\n",
      " - TD3 Sharpe Ratio:  0.16059318071872558\n",
      " - Trading from:  20181003 to  20190104\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1622693.226801916\n",
      "end_total_asset:1641640.784634199\n",
      "total_reward:18947.557832282968\n",
      "total_cost:  1111.1735406540654\n",
      "total trades:  144\n",
      "Sharpe:  0.314337493739414\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20181003\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.106788158416748  minutes\n",
      " - PPO Validation from:  20181003 to  20190104\n",
      " - PPO Sharpe Ratio:  -0.41048094024621296\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.459615878264109  minutes\n",
      " - SAC Validation from:  20181003 to  20190104\n",
      " - SAC Sharpe Ratio:  -0.3660831771279706\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.8324276765187582  minutes\n",
      " - TD3 Validation from:  20181003 to  20190104\n",
      " - TD3 Sharpe Ratio:  -0.36017836601401687\n",
      " - Trading from:  20190104 to  20190405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1641640.784634199\n",
      "end_total_asset:1847173.8534941997\n",
      "total_reward:205533.0688600007\n",
      "total_cost:  2038.0651400000002\n",
      "total trades:  1044\n",
      "Sharpe:  0.5607343431853744\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190104\n",
      " - PPO Training\n",
      " - Training time (PPO):  7.622802706559499  minutes\n",
      " - PPO Validation from:  20190104 to  20190405\n",
      " - PPO Sharpe Ratio:  0.18378958898511985\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.569682554403941  minutes\n",
      " - SAC Validation from:  20190104 to  20190405\n",
      " - SAC Sharpe Ratio:  -0.0014268655931996379\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.6716609398523967  minutes\n",
      " - TD3 Validation from:  20190104 to  20190405\n",
      " - TD3 Sharpe Ratio:  0.1470357269009076\n",
      " - Trading from:  20190405 to  20190708\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1847173.8534941997\n",
      "end_total_asset:1851529.8275031354\n",
      "total_reward:4355.9740089357365\n",
      "total_cost:  878.9157115180992\n",
      "total trades:  135\n",
      "Sharpe:  0.3208318682870364\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190405\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.357721698284149  minutes\n",
      " - PPO Validation from:  20190405 to  20190708\n",
      " - PPO Sharpe Ratio:  0.24975819537102456\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.250461526711782  minutes\n",
      " - SAC Validation from:  20190405 to  20190708\n",
      " - SAC Sharpe Ratio:  0.1397972754391556\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.5520249644915263  minutes\n",
      " - TD3 Validation from:  20190405 to  20190708\n",
      " - TD3 Sharpe Ratio:  0.3551542297063251\n",
      " - Trading from:  20190708 to  20191004\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1851529.8275031354\n",
      "end_total_asset:1854729.5901888069\n",
      "total_reward:3199.7626856714487\n",
      "total_cost:  2759.7706867799757\n",
      "total trades:  286\n",
      "Sharpe:  0.05776630678070827\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190708\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.414696749051412  minutes\n",
      " - PPO Validation from:  20190708 to  20191004\n",
      " - PPO Sharpe Ratio:  0.03290064637925952\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.671099623044332  minutes\n",
      " - SAC Validation from:  20190708 to  20191004\n",
      " - SAC Sharpe Ratio:  0.04455574978555053\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.784906268119812  minutes\n",
      " - TD3 Validation from:  20190708 to  20191004\n",
      " - TD3 Sharpe Ratio:  0.10112630178406731\n",
      " - Trading from:  20191004 to  20200106\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1854729.5901888069\n",
      "end_total_asset:1852868.4681888067\n",
      "total_reward:-1861.1220000002068\n",
      "total_cost:  375.11699999999996\n",
      "total trades:  48\n",
      "Sharpe:  -0.41954197314515324\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20191004\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.418758563200632  minutes\n",
      " - PPO Validation from:  20191004 to  20200106\n",
      " - PPO Sharpe Ratio:  -0.450888822764595\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.6785055041313175  minutes\n",
      " - SAC Validation from:  20191004 to  20200106\n",
      " - SAC Sharpe Ratio:  -0.4316309562089604\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.7927846948305766  minutes\n",
      " - TD3 Validation from:  20191004 to  20200106\n",
      " - TD3 Sharpe Ratio:  -0.41182812446172956\n",
      " - Trading from:  20200106 to  20200406\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1852868.4681888067\n",
      "end_total_asset:1809390.7786888066\n",
      "total_reward:-43477.689500000095\n",
      "total_cost:  2024.8284999999996\n",
      "total trades:  240\n",
      "Sharpe:  -0.41869599178688915\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20200106\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.551103830337524  minutes\n",
      " - PPO Validation from:  20200106 to  20200406\n",
      " - PPO Sharpe Ratio:  -0.4360063417950177\n",
      " - SAC Training\n",
      " - Training time (SAC):  4.567802162965139  minutes\n",
      " - SAC Validation from:  20200106 to  20200406\n",
      " - SAC Sharpe Ratio:  -0.45768307733903896\n",
      " - TD3 Training\n",
      " - Training time (TD3):  1.821803375085195  minutes\n",
      " - TD3 Validation from:  20200106 to  20200406\n",
      " - TD3 Sharpe Ratio:  -0.4183471535508412\n",
      " - Trading from:  20200406 to  20200707\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1809390.7786888066\n",
      "end_total_asset:1816351.2646888066\n",
      "total_reward:6960.4860000000335\n",
      "total_cost:  847.4250000000001\n",
      "total trades:  126\n",
      "Sharpe:  0.2751447999200658\n",
      "Ensemble Strategy took:  292.886067489783  minutes\n"
     ]
    }
   ],
   "source": [
    "run_ensemble_strategy_new(df=df, unique_trade_date=unique_trade_date, rebalance_window=rebalance_window, validation_window=validation_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "wired-burlington",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-03T05:00:43.663960Z",
     "iopub.status.busy": "2021-04-03T05:00:43.662743Z",
     "iopub.status.idle": "2021-04-03T07:46:32.346383Z",
     "shell.execute_reply": "2021-04-03T07:46:32.345691Z"
    },
    "papermill": {
     "duration": 9948.72573,
     "end_time": "2021-04-03T07:46:32.346561",
     "exception": false,
     "start_time": "2021-04-03T05:00:43.620831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20151002\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8374975204467774  minutes\n",
      " - A2C Validation from:  20151002 to  20160104\n",
      " - A2C Sharpe Ratio:  0.1413520573592471\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.2211330731709795  minutes\n",
      " - PPO Validation from:  20151002 to  20160104\n",
      " - PPO Sharpe Ratio:  0.02725676683115885\n",
      " - DDPG Training\n",
      "WARNING:tensorflow:From C:\\Users\\cloie\\anaconda3\\envs\\cs551\\lib\\site-packages\\stable_baselines\\ddpg\\ddpg.py:446: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\cloie\\anaconda3\\envs\\cs551\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:297: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      " - Training time (DDPG):  1.030861763159434  minutes\n",
      " - DDPG Validation from:  20151002 to  20160104\n",
      " - Trading from:  20160104 to  20160405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1000000\n",
      "end_total_asset:1083886.522398797\n",
      "total_reward:83886.5223987971\n",
      "total_cost:  5628.801405718654\n",
      "total trades:  1380\n",
      "Sharpe:  0.2615264613791322\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20160104\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8382044394810995  minutes\n",
      " - A2C Validation from:  20160104 to  20160405\n",
      " - A2C Sharpe Ratio:  0.06613296077279802\n",
      " - PPO Training\n",
      " - Training time (PPO):  5.890278895696004  minutes\n",
      " - PPO Validation from:  20160104 to  20160405\n",
      " - PPO Sharpe Ratio:  0.03562142310380177\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.0650198737780252  minutes\n",
      " - DDPG Validation from:  20160104 to  20160405\n",
      " - Trading from:  20160405 to  20160705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1083886.522398797\n",
      "end_total_asset:1141803.0987775563\n",
      "total_reward:57916.57637875923\n",
      "total_cost:  1413.7454921931508\n",
      "total trades:  772\n",
      "Sharpe:  0.19186451617058842\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20160405\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.887360362211863  minutes\n",
      " - A2C Validation from:  20160405 to  20160705\n",
      " - A2C Sharpe Ratio:  0.027865049986065104\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.409231034914653  minutes\n",
      " - PPO Validation from:  20160405 to  20160705\n",
      " - PPO Sharpe Ratio:  0.009804821569305152\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.1213356256484985  minutes\n",
      " - DDPG Validation from:  20160405 to  20160705\n",
      " - Trading from:  20160705 to  20161003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1141803.0987775563\n",
      "end_total_asset:1111764.0895562028\n",
      "total_reward:-30039.009221353568\n",
      "total_cost:  3572.9035275182246\n",
      "total trades:  1360\n",
      "Sharpe:  -0.10621540674241217\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20160705\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8628202040990194  minutes\n",
      " - A2C Validation from:  20160705 to  20161003\n",
      " - A2C Sharpe Ratio:  -0.07064278150809813\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.416624534130096  minutes\n",
      " - PPO Validation from:  20160705 to  20161003\n",
      " - PPO Sharpe Ratio:  -0.04851612161489906\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.2201709032058716  minutes\n",
      " - DDPG Validation from:  20160705 to  20161003\n",
      " - Trading from:  20161003 to  20170103\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1111764.0895562028\n",
      "end_total_asset:1173545.8498584812\n",
      "total_reward:61781.760302278446\n",
      "total_cost:  1331.644187461727\n",
      "total trades:  1236\n",
      "Sharpe:  0.24131446313438357\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20161003\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8496912558873495  minutes\n",
      " - A2C Validation from:  20161003 to  20170103\n",
      " - A2C Sharpe Ratio:  0.4803340455851338\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.3208293437957765  minutes\n",
      " - PPO Validation from:  20161003 to  20170103\n",
      " - PPO Sharpe Ratio:  0.5064428831246521\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.214074929555257  minutes\n",
      " - DDPG Validation from:  20161003 to  20170103\n",
      " - Trading from:  20170103 to  20170404\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1173545.8498584812\n",
      "end_total_asset:1097785.6892391283\n",
      "total_reward:-75760.16061935294\n",
      "total_cost:  524.5539978981877\n",
      "total trades:  1104\n",
      "Sharpe:  -0.26706589594656605\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20170103\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8815022945404052  minutes\n",
      " - A2C Validation from:  20170103 to  20170404\n",
      " - A2C Sharpe Ratio:  0.40525474688588253\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.6346625248591105  minutes\n",
      " - PPO Validation from:  20170103 to  20170404\n",
      " - PPO Sharpe Ratio:  0.4014391024207844\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.3491018255551657  minutes\n",
      " - DDPG Validation from:  20170103 to  20170404\n",
      " - Trading from:  20170404 to  20170705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1097785.6892391283\n",
      "end_total_asset:1105068.987672476\n",
      "total_reward:7283.298433347838\n",
      "total_cost:  4977.523638747883\n",
      "total trades:  1119\n",
      "Sharpe:  0.059051983452459045\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20170404\n",
      " - A2C Training\n",
      " - Training time (A2C):  2.0356929540634154  minutes\n",
      " - A2C Validation from:  20170404 to  20170705\n",
      " - A2C Sharpe Ratio:  0.14073400779494014\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.523607794443766  minutes\n",
      " - PPO Validation from:  20170404 to  20170705\n",
      " - PPO Sharpe Ratio:  0.20993907933891195\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.1802043676376344  minutes\n",
      " - DDPG Validation from:  20170404 to  20170705\n",
      " - Trading from:  20170705 to  20171003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1105068.987672476\n",
      "end_total_asset:1180804.6894513902\n",
      "total_reward:75735.70177891408\n",
      "total_cost:  7946.2089107026295\n",
      "total trades:  1468\n",
      "Sharpe:  0.4830026305233835\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20170705\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8736417611440024  minutes\n",
      " - A2C Validation from:  20170705 to  20171003\n",
      " - A2C Sharpe Ratio:  0.21020199626509434\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.3340260744094845  minutes\n",
      " - PPO Validation from:  20170705 to  20171003\n",
      " - PPO Sharpe Ratio:  0.20249815546405284\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.1264340281486511  minutes\n",
      " - DDPG Validation from:  20170705 to  20171003\n",
      " - Trading from:  20171003 to  20180103\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1180804.6894513902\n",
      "end_total_asset:1329724.2368458554\n",
      "total_reward:148919.5473944652\n",
      "total_cost:  1684.5387777842768\n",
      "total trades:  1273\n",
      "Sharpe:  0.9258268532279855\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20171003\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.9187270879745484  minutes\n",
      " - A2C Validation from:  20171003 to  20180103\n",
      " - A2C Sharpe Ratio:  0.5042374824925315\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.171497050921122  minutes\n",
      " - PPO Validation from:  20171003 to  20180103\n",
      " - PPO Sharpe Ratio:  0.36901927015109776\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.1050456007321676  minutes\n",
      " - DDPG Validation from:  20171003 to  20180103\n",
      " - Trading from:  20180103 to  20180405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1329724.2368458554\n",
      "end_total_asset:1358362.3395289215\n",
      "total_reward:28638.102683066158\n",
      "total_cost:  1992.839626919705\n",
      "total trades:  336\n",
      "Sharpe:  0.18496001412701019\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180103\n",
      " - A2C Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Training time (A2C):  1.8683258096377056  minutes\n",
      " - A2C Validation from:  20180103 to  20180405\n",
      " - A2C Sharpe Ratio:  -0.03489720698522685\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.476804439226786  minutes\n",
      " - PPO Validation from:  20180103 to  20180405\n",
      " - PPO Sharpe Ratio:  -0.00224014077434724\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.125878357887268  minutes\n",
      " - DDPG Validation from:  20180103 to  20180405\n",
      " - Trading from:  20180405 to  20180705\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1358362.3395289215\n",
      "end_total_asset:1350102.8773368986\n",
      "total_reward:-8259.46219202294\n",
      "total_cost:  7377.30888626756\n",
      "total trades:  1135\n",
      "Sharpe:  -0.02677664885475552\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180405\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.968150273958842  minutes\n",
      " - A2C Validation from:  20180405 to  20180705\n",
      " - A2C Sharpe Ratio:  -0.12062086269787785\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.438431445757548  minutes\n",
      " - PPO Validation from:  20180405 to  20180705\n",
      " - PPO Sharpe Ratio:  0.021236710871838756\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.01377565463384  minutes\n",
      " - DDPG Validation from:  20180405 to  20180705\n",
      " - Trading from:  20180705 to  20181003\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1350102.8773368986\n",
      "end_total_asset:1380180.640565823\n",
      "total_reward:30077.76322892448\n",
      "total_cost:  6624.141827652395\n",
      "total trades:  977\n",
      "Sharpe:  0.29089249525784594\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20180705\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8693185607592264  minutes\n",
      " - A2C Validation from:  20180705 to  20181003\n",
      " - A2C Sharpe Ratio:  0.18493516320665168\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.2345987757047014  minutes\n",
      " - PPO Validation from:  20180705 to  20181003\n",
      " - PPO Sharpe Ratio:  0.17455177087756785\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.009324606259664  minutes\n",
      " - DDPG Validation from:  20180705 to  20181003\n",
      " - Trading from:  20181003 to  20190104\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1380180.640565823\n",
      "end_total_asset:1391526.2317463874\n",
      "total_reward:11345.59118056437\n",
      "total_cost:  1047.206371666614\n",
      "total trades:  176\n",
      "Sharpe:  0.21870412392077904\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  171.09407156310158\n",
      " - Model training from:  20090000 to  20181003\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.875112001101176  minutes\n",
      " - A2C Validation from:  20181003 to  20190104\n",
      " - A2C Sharpe Ratio:  -0.3606353492090903\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.281269633769989  minutes\n",
      " - PPO Validation from:  20181003 to  20190104\n",
      " - PPO Sharpe Ratio:  -0.3901233504107725\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.0372447729110719  minutes\n",
      " - DDPG Validation from:  20181003 to  20190104\n",
      " - Trading from:  20190104 to  20190405\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1391526.2317463874\n",
      "end_total_asset:1471687.6851449313\n",
      "total_reward:80161.45339854388\n",
      "total_cost:  7950.713420034798\n",
      "total trades:  1447\n",
      "Sharpe:  0.2109561865587335\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190104\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.8460819999376932  minutes\n",
      " - A2C Validation from:  20190104 to  20190405\n",
      " - A2C Sharpe Ratio:  0.09701217724365502\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.306527372201284  minutes\n",
      " - PPO Validation from:  20190104 to  20190405\n",
      " - PPO Sharpe Ratio:  0.11246795282227463\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.0550846060117085  minutes\n",
      " - DDPG Validation from:  20190104 to  20190405\n",
      " - Trading from:  20190405 to  20190708\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1471687.6851449313\n",
      "end_total_asset:1479479.9920401352\n",
      "total_reward:7792.306895203888\n",
      "total_cost:  1619.153588679237\n",
      "total trades:  128\n",
      "Sharpe:  0.2848385065510377\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190405\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.9464780648549398  minutes\n",
      " - A2C Validation from:  20190405 to  20190708\n",
      " - A2C Sharpe Ratio:  0.33282334551281034\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.439440015951792  minutes\n",
      " - PPO Validation from:  20190405 to  20190708\n",
      " - PPO Sharpe Ratio:  0.30165460905416164\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.060831367969513  minutes\n",
      " - DDPG Validation from:  20190405 to  20190708\n",
      " - Trading from:  20190708 to  20191004\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1479479.9920401352\n",
      "end_total_asset:1483877.9267103842\n",
      "total_reward:4397.934670249\n",
      "total_cost:  1697.7809759675035\n",
      "total trades:  340\n",
      "Sharpe:  0.05184774763925706\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20190708\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.889594570795695  minutes\n",
      " - A2C Validation from:  20190708 to  20191004\n",
      " - A2C Sharpe Ratio:  0.01910746321321217\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.33261874516805  minutes\n",
      " - PPO Validation from:  20190708 to  20191004\n",
      " - PPO Sharpe Ratio:  0.17251922200224298\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.0917909542719524  minutes\n",
      " - DDPG Validation from:  20190708 to  20191004\n",
      " - Trading from:  20191004 to  20200106\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1483877.9267103842\n",
      "end_total_asset:1482696.3586119483\n",
      "total_reward:-1181.568098435877\n",
      "total_cost:  350.11840525235414\n",
      "total trades:  66\n",
      "Sharpe:  -0.2344937026453065\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20191004\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.9232692877451578  minutes\n",
      " - A2C Validation from:  20191004 to  20200106\n",
      " - A2C Sharpe Ratio:  -0.39272929230471554\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.474452269077301  minutes\n",
      " - PPO Validation from:  20191004 to  20200106\n",
      " - PPO Sharpe Ratio:  -0.4346061618485752\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.0677058378855386  minutes\n",
      " - DDPG Validation from:  20191004 to  20200106\n",
      " - Trading from:  20200106 to  20200406\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1482696.3586119483\n",
      "end_total_asset:1462218.896744245\n",
      "total_reward:-20477.461867703358\n",
      "total_cost:  1026.716132934246\n",
      "total trades:  135\n",
      "Sharpe:  -0.43437221252948743\n",
      "--------------------------------------------------\n",
      " - Turbulence_threshold:  96.08032158358223\n",
      " - Model training from:  20090000 to  20200106\n",
      " - A2C Training\n",
      " - Training time (A2C):  1.9270158131917319  minutes\n",
      " - A2C Validation from:  20200106 to  20200406\n",
      " - A2C Sharpe Ratio:  -0.3503633769645128\n",
      " - PPO Training\n",
      " - Training time (PPO):  6.4823709925015764  minutes\n",
      " - PPO Validation from:  20200106 to  20200406\n",
      " - PPO Sharpe Ratio:  -0.4220003020414808\n",
      " - DDPG Training\n",
      " - Training time (DDPG):  1.1141107042630514  minutes\n",
      " - DDPG Validation from:  20200106 to  20200406\n",
      " - Trading from:  20200406 to  20200707\n",
      "--------------------------------------------------\n",
      "previous_total_asset:1462218.896744245\n",
      "end_total_asset:1466125.9493962289\n",
      "total_reward:3907.0526519839186\n",
      "total_cost:  446.85931751129164\n",
      "total trades:  99\n",
      "Sharpe:  0.21158835454205602\n",
      "Ensemble Strategy took:  168.7719552755356  minutes\n"
     ]
    }
   ],
   "source": [
    "run_ensemble_strategy(df=df, \n",
    "                      unique_trade_date= unique_trade_date,\n",
    "                      rebalance_window = rebalance_window,\n",
    "                      validation_window=validation_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-sense",
   "metadata": {
    "papermill": {
     "duration": 0.074992,
     "end_time": "2021-04-03T07:46:32.497986",
     "exception": false,
     "start_time": "2021-04-03T07:46:32.422994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1 id=\"references\" style=\"color:#eef666; background:#567fb7; border:0.5px dotted;\"> \n",
    "    <center>References\n",
    "        <a class=\"anchor-link\" href=\"#references\" target=\"_self\">¶</a>\n",
    "    </center>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-puzzle",
   "metadata": {
    "papermill": {
     "duration": 0.074825,
     "end_time": "2021-04-03T07:46:32.648223",
     "exception": false,
     "start_time": "2021-04-03T07:46:32.573398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy.<br>\n",
    "In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10126.704499,
   "end_time": "2021-04-03T07:46:34.243540",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-04-03T04:57:47.539041",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
